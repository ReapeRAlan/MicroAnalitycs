"""
Frontend para Ollama - Interfaz de Chat para Predicci√≥n de Demanda
================================================================

Aplicaci√≥n Streamlit que proporciona una interfaz de usuario moderna
para interactuar con el sistema de predicci√≥n de demanda a trav√©s de Ollama.
"""

import streamlit as st
import asyncio
import json
import requests
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import sys
import os

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Agregar el directorio padre al path para importar m√≥dulos
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Importaciones b√°sicas sin Ollama por ahora
try:
    from chatbot.communication_schema import (
        PredictionScope, ModelType, RequestType
    )
    OLLAMA_AVAILABLE = True
except ImportError as e:
    print(f"Ollama integration not available: {e}")
    OLLAMA_AVAILABLE = False
    # Definir enums b√°sicos como fallback
    class PredictionScope:
        SINGLE_PRODUCT = "single_product"
        CATEGORY = "category" 
        BUSINESS = "business"
        MARKET = "market"
    
    class ModelType:
        AUTO_SELECT = "auto_select"
        LINEAR = "linear"
        POLYNOMIAL = "polynomial"


# Configuraci√≥n de la p√°gina
# st.set_page_config(
#     page_title="MicroAnalytics - Chat de Predicci√≥n",
#     page_icon="ü§ñ",
#     layout="wide",
#     initial_sidebar_state="expanded"
# )

# CSS optimizado para m√°xima legibilidad y contraste
st.markdown("""
<style>
/* Reset y base */
.main {
    padding-top: 1rem;
}

/* Estilos base para mensajes del chat con contraste m√°ximo */
.stChatMessage {
    border-radius: 12px;
    padding: 20px;
    margin: 15px 0;
    font-size: 16px;
    line-height: 1.7;
    border: 2px solid transparent;
}

.user-message {
    background-color: #ffffff !important;
    border: 3px solid #1976d2 !important;
    color: #0d47a1 !important;
    font-weight: 600;
    box-shadow: 0 3px 10px rgba(25, 118, 210, 0.2);
}

.user-message strong {
    color: #0d47a1 !important;
    font-weight: 700;
}

.assistant-message {
    background-color: #ffffff !important;
    border: 3px solid #7b1fa2 !important;
    color: #212121 !important;
    box-shadow: 0 4px 15px rgba(123, 31, 162, 0.15);
}

/* Forzar color negro en todos los elementos del asistente */
.assistant-message,
.assistant-message *,
.assistant-message p,
.assistant-message div,
.assistant-message span,
.assistant-message li,
.assistant-message td,
.assistant-message th {
    color: #000000 !important;
    font-weight: 500 !important;
}

/* T√≠tulos con contraste extremo */
.assistant-message h1,
.assistant-message h2,
.assistant-message h3,
.assistant-message h4,
.assistant-message h5,
.assistant-message h6 {
    color: #000000 !important;
    font-weight: 800 !important;
    margin: 20px 0 15px 0 !important;
    text-shadow: none !important;
    background-color: #e8eaf6 !important;
    padding: 8px 12px !important;
    border-radius: 6px !important;
    border-left: 4px solid #3f51b5 !important;
}

/* Texto enfatizado negro s√≥lido */
.assistant-message strong,
.assistant-message b {
    color: #000000 !important;
    font-weight: 800 !important;
    background-color: #fff3e0 !important;
    padding: 2px 4px !important;
    border-radius: 3px !important;
}

/* Enlaces completamente visibles */
.assistant-message a {
    color: #000000 !important;
    text-decoration: underline !important;
    font-weight: 700 !important;
    background-color: #e3f2fd !important;
    padding: 2px 4px !important;
    border-radius: 3px !important;
}

/* Indicadores de confianza con contraste extremo */
.confidence-high { 
    color: #000000 !important; 
    font-weight: 900 !important;
    background-color: #c8e6c9 !important;
    padding: 6px 12px !important;
    border-radius: 6px !important;
    border: 3px solid #2e7d32 !important;
    text-shadow: none !important;
}

.confidence-medium { 
    color: #000000 !important; 
    font-weight: 900 !important;
    background-color: #ffe0b2 !important;
    padding: 6px 12px !important;
    border-radius: 6px !important;
    border: 3px solid #f57c00 !important;
    text-shadow: none !important;
}

.confidence-low { 
    color: #000000 !important; 
    font-weight: 900 !important;
    background-color: #ffcdd2 !important;
    padding: 6px 12px !important;
    border-radius: 6px !important;
    border: 3px solid #d32f2f !important;
    text-shadow: none !important;
}

/* Listas completamente negras */
.assistant-message ul,
.assistant-message ol {
    color: #000000 !important;
    margin: 15px 0 !important;
}

.assistant-message ul li,
.assistant-message ol li {
    color: #000000 !important;
    font-weight: 600 !important;
    margin: 8px 0 !important;
    padding-left: 10px !important;
}

.assistant-message ul li::marker,
.assistant-message ol li::marker {
    color: #000000 !important;
}

/* C√≥digo completamente visible */
.assistant-message code,
.assistant-message pre {
    background-color: #f5f5f5 !important;
    color: #000000 !important;
    border: 2px solid #666666 !important;
    border-radius: 4px !important;
    padding: 8px !important;
    font-weight: 600 !important;
}

/* Forzar estilos en todos los elementos de Streamlit */
div[data-testid="stMarkdown"],
div[data-testid="stMarkdown"] *,
.stMarkdown,
.stMarkdown *,
.st-emotion-cache-acwcvw,
.st-emotion-cache-acwcvw *,
.st-emotion-cache-1sdpuyj,
.st-emotion-cache-1sdpuyj * {
    color: #000000 !important;
    font-weight: 500 !important;
}

/* Selectores espec√≠ficos para elementos problem√°ticos */
.assistant-message div[data-testid="stMarkdown"] p,
.assistant-message div[data-testid="stMarkdown"] div,
.assistant-message div[data-testid="stMarkdown"] span,
.assistant-message div[data-testid="stMarkdown"] li,
.assistant-message .stMarkdown p,
.assistant-message .stMarkdown div,
.assistant-message .stMarkdown span,
.assistant-message .stMarkdown li {
    color: #000000 !important;
    font-weight: 500 !important;
}

/* Sidebar mejorado */
.sidebar-section {
    margin-bottom: 30px;
    padding: 20px;
    background-color: #ffffff;
    border-radius: 10px;
    border: 2px solid #e0e0e0;
    color: #000000 !important;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
}

.sidebar-section * {
    color: #000000 !important;
}

/* Forzar todo a negro como √∫ltimo recurso */
.assistant-message [class*="st-emotion"],
.assistant-message [class*="e1rzn78k"],
.assistant-message [class*="erovr38"] {
    color: #000000 !important;
}

/* Asegurar que elementos espec√≠ficos sean negros */
div.stChatMessage.assistant-message * {
    color: #000000 !important;
}

/* Override para cualquier clase que Streamlit pueda agregar */
.assistant-message [class] {
    color: #000000 !important;
}

/* Modo oscuro deshabilitado para m√°ximo contraste */
@media (prefers-color-scheme: dark) {
    .assistant-message,
    .assistant-message * {
        background-color: #ffffff !important;
        color: #000000 !important;
    }
}
</style>
""", unsafe_allow_html=True)


class ChatbotFrontend:
    """Clase principal para el frontend del chatbot"""
    
    def __init__(self):
        self.backend_url = "http://localhost:8000"  # URL del backend FastAPI
        self.session_id = self._get_session_id()
        self.ollama_client = None
        self.interpreter = None
        
        # Inicializar estado de la sesi√≥n
        if 'messages' not in st.session_state:
            st.session_state.messages = []
        if 'prediction_history' not in st.session_state:
            st.session_state.prediction_history = []
        if 'current_context' not in st.session_state:
            st.session_state.current_context = {}
        if 'tool_results' not in st.session_state:
            st.session_state.tool_results = {
                'recent_predictions': [],
                'recent_comparisons': [],
                'recent_analysis': [],
                'last_action': None,
                'last_results': None
            }
    
    def _get_session_id(self) -> str:
        """Obtener o crear ID de sesi√≥n"""
        if 'session_id' not in st.session_state:
            st.session_state.session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        return st.session_state.session_id
    
    async def _init_ollama_client(self):
        """Inicializar cliente de Ollama con detecci√≥n autom√°tica de modelos"""
        if not OLLAMA_AVAILABLE:
            logger.warning("Ollama integration no disponible")
            return False
            
        if self.ollama_client is None:
            try:
                from chatbot.ollama_integration import OllamaClient, OllamaConfig
                
                # URL fija de Ollama
                ollama_url = "https://3200-34-168-28-225.ngrok-free.app"
                
                # Crear configuraci√≥n temporal para detectar modelos
                temp_config = OllamaConfig(
                    base_url=ollama_url,
                    model_name="llama3.2",  # Modelo por defecto
                    timeout=30,
                    max_tokens=1000,
                    temperature=0.7
                )
                
                # Crear cliente temporal
                temp_client = OllamaClient(temp_config)
                
                # Detectar modelos disponibles
                available_models = await self._detect_available_models(temp_client, ollama_url)
                
                if not available_models:
                    logger.warning("No se pudieron detectar modelos en Ollama")
                    return False
                
                # Seleccionar el mejor modelo
                best_model = self._select_best_model(available_models)
                logger.info(f"Usando modelo: {best_model} de {available_models}")
                
                # Crear configuraci√≥n final
                final_config = OllamaConfig(
                    base_url=ollama_url,
                    model_name=best_model,
                    timeout=120,
                    max_tokens=1000,
                    temperature=0.7
                )
                
                # Crear cliente final
                self.ollama_client = OllamaClient(final_config)
                
                # Verificar conexi√≥n final
                if await self.ollama_client.check_connection():
                    logger.info(f"Ollama conectado exitosamente con modelo {best_model}")
                    st.session_state.ollama_connected = True
                    st.session_state.ollama_model_used = best_model
                    return True
                else:
                    logger.warning("No se pudo conectar con Ollama")
                    self.ollama_client = None
                    return False
                    
            except ImportError as e:
                logger.error(f"Error importando Ollama (posiblemente falta aiohttp): {e}")
                return False
            except Exception as e:
                logger.error(f"Error inicializando Ollama: {e}")
                self.ollama_client = None
                return False
        
        return True

    async def _detect_available_models(self, client, base_url):
        """Detectar modelos disponibles en Ollama"""
        try:
            # Intentar importar aiohttp
            try:
                import aiohttp
            except ImportError:
                logger.error("aiohttp no est√° disponible. Inst√°lalo con: pip install aiohttp")
                return []
            
            async with aiohttp.ClientSession() as session:
                headers = {
                    "ngrok-skip-browser-warning": "true",
                    "Content-Type": "application/json"
                }
                
                async with session.get(
                    f"{base_url}/api/tags",
                    timeout=aiohttp.ClientTimeout(total=10),
                    headers=headers
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        models = [model.get('name', '').split(':')[0] for model in data.get('models', [])]
                        models = [m for m in models if m]  # Filtrar nombres vac√≠os
                        logger.info(f"Modelos detectados: {models}")
                        return models
                    else:
                        logger.warning(f"Error al obtener modelos: {response.status}")
                        return []
        except Exception as e:
            logger.error(f"Error detectando modelos: {e}")
            # Fallback con modelos comunes
            return ['llama3.2', 'llama3.1', 'llama3']

    def _select_best_model(self, available_models):
        """Seleccionar el mejor modelo basado en prioridad"""
        # Prioridad de modelos (del mejor al menos preferido)
        priority_models = [
            'llama3.2',
            'llama3.1', 
            'llama3',
            'llama2',
            'codellama',
            'mistral',
            'gemma',
            'phi',
            'qwen'
        ]
        
        # Buscar el primer modelo de la lista de prioridad que est√© disponible
        for preferred in priority_models:
            for available in available_models:
                if preferred.lower() in available.lower():
                    return available
        
        # Si no se encuentra ninguno preferido, usar el primero disponible
        if available_models:
            return available_models[0]
        
        # Fallback
        return "llama3.2"
    
    def render_sidebar(self):
        """Renderizar barra lateral con configuraciones"""
        st.sidebar.header("ü§ñ Configuraci√≥n del Chat")
        
        with st.sidebar.container():
            st.markdown('<div class="sidebar-section">', unsafe_allow_html=True)
            st.subheader("üîó Estado de Ollama")
            
            # Estado de conexi√≥n
            if st.session_state.get('ollama_connected', False):
                st.success(f"‚úÖ Conectado - Modelo: {st.session_state.get('ollama_model_used', 'N/A')}")
            else:
                st.warning("‚ö†Ô∏è No conectado")
            
            # URL de Ollama (fija)
            st.info("üåê URL: https://3200-34-168-28-225.ngrok-free.app")
            
            # Bot√≥n para reconectar
            if st.button("üîÑ Reconectar Ollama"):
                self.ollama_client = None
                st.session_state.ollama_connected = False
                try:
                    connected = asyncio.run(self._init_ollama_client())
                    if connected:
                        st.success("‚úÖ Reconectado exitosamente")
                        st.rerun()
                    else:
                        st.error("‚ùå Error al reconectar")
                except Exception as e:
                    st.error(f"‚ùå Error: {e}")
            
            st.markdown('</div>', unsafe_allow_html=True)
            
            # Configuraci√≥n de predicciones
            st.markdown('<div class="sidebar-section">', unsafe_allow_html=True)
            st.subheader("üìä Configuraci√≥n de Predicciones")
            
            # D√≠as para predicci√≥n
            prediction_days = st.slider(
                "D√≠as a predecir",
                min_value=7,
                max_value=90,
                value=st.session_state.get('prediction_days', 30),
                step=7,
                help="N√∫mero de d√≠as hacia el futuro para las predicciones"
            )
            st.session_state.prediction_days = prediction_days
            
            # Incluir intervalos de confianza
            include_confidence = st.checkbox(
                "Incluir intervalos de confianza",
                value=st.session_state.get('include_confidence', True),
                help="Mostrar rangos de confianza en las predicciones"
            )
            st.session_state.include_confidence = include_confidence
            
            # Usar cach√©
            use_cache = st.checkbox(
                "Usar cach√© de modelos",
                value=st.session_state.get('use_cache', True),
                help="Acelerar predicciones usando modelos en cach√©"
            )
            st.session_state.use_cache = use_cache
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # Historial de predicciones
        st.sidebar.header("ÔøΩ Historial")
        
        with st.sidebar.container():
            st.markdown('<div class="sidebar-section">', unsafe_allow_html=True)
            
            # Alcance de predicci√≥n
            scope_options = {
                "Producto √∫nico": PredictionScope.SINGLE_PRODUCT,
                "Categor√≠a": PredictionScope.CATEGORY,
                "Negocio": PredictionScope.BUSINESS,
                "Mercado": PredictionScope.MARKET
            }
            
            selected_scope = st.selectbox(
                "Alcance",
                options=list(scope_options.keys()),
                help="Alcance de la predicci√≥n"
            )
            st.session_state.prediction_scope = scope_options[selected_scope]
            
            # D√≠as de predicci√≥n
            prediction_days = st.slider(
                "D√≠as a predecir",
                min_value=1,
                max_value=365,
                value=30,
                help="N√∫mero de d√≠as hacia el futuro"
            )
            st.session_state.prediction_days = prediction_days
            
            # Tipo de modelo
            model_options = {
                "Auto-selecci√≥n": ModelType.AUTO_SELECT,
                "Lineal": ModelType.LINEAR,
                "Polinomial": ModelType.POLYNOMIAL
            }
            
            selected_model_type = st.selectbox(
                "Tipo de modelo",
                options=list(model_options.keys()),
                help="Modelo de ML a utilizar"
            )
            st.session_state.model_type = model_options[selected_model_type]
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # Historial de predicciones
        st.sidebar.header("üìà Historial")
        
        if st.session_state.prediction_history:
            for i, pred in enumerate(st.session_state.prediction_history[-5:]):
                with st.sidebar.expander(f"Predicci√≥n {i+1}"):
                    st.write(f"**Fecha:** {pred['timestamp']}")
                    st.write(f"**Confianza:** {pred['confidence']:.1%}")
                    st.write(f"**Modelo:** {pred['model']}")
        else:
            st.sidebar.info("No hay predicciones a√∫n")
        
        # Limpiar historial
        if st.sidebar.button("üóëÔ∏è Limpiar Chat"):
            st.session_state.messages = []
            st.session_state.prediction_history = []
            st.rerun()
    
    def render_chat_interface(self):
        """Renderizar interfaz principal de chat"""
        st.header("ü§ñ Asistente de Predicci√≥n de Demanda")
        st.markdown("Pregunta sobre predicciones, an√°lisis de tendencias y insights de tu negocio.")
        
        # Contenedor para mensajes
        chat_container = st.container()
        
        with chat_container:
            # Mostrar historial de mensajes
            for message in st.session_state.messages:
                self._render_message(message)
        
        # Input para nuevos mensajes
        user_input = st.chat_input("Escribe tu pregunta sobre predicci√≥n de demanda...")
        
        if user_input:
            # Agregar mensaje del usuario
            st.session_state.messages.append({
                "role": "user",
                "content": user_input,
                "timestamp": datetime.now()
            })
            
            # Procesar mensaje
            with st.spinner("Analizando y generando respuesta..."):
                response = self._process_user_message(user_input)
            
            # Agregar respuesta del asistente
            st.session_state.messages.append({
                "role": "assistant",
                "content": response,
                "timestamp": datetime.now()
            })
            
            st.rerun()
    
    def _render_message(self, message: Dict[str, Any]):
        """Renderizar un mensaje individual"""
        timestamp = message['timestamp'].strftime("%H:%M")
        
        if message['role'] == 'user':
            st.markdown(f"""
            <div class="stChatMessage user-message">
                <strong>T√∫ ({timestamp}):</strong> {message['content']}
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
            <div class="stChatMessage assistant-message">
                <strong>Asistente ({timestamp}):</strong> {message['content']}
            </div>
            """, unsafe_allow_html=True)
            
            # Si hay datos de predicci√≥n, mostrar visualizaci√≥n
            if 'prediction_data' in message:
                self._render_prediction_visualization(message['prediction_data'])
    
    def _render_prediction_visualization(self, prediction_data: Dict[str, Any]):
        """Renderizar visualizaci√≥n de predicci√≥n"""
        try:
            if not prediction_data or 'predicciones' not in prediction_data:
                return
            
            # Crear gr√°fico simple con Plotly
            import plotly.graph_objects as go
            
            predicciones = prediction_data.get('predicciones', [])
            if not predicciones:
                return
                
            dias = list(range(1, len(predicciones) + 1))
            
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=dias,
                y=predicciones,
                mode='lines+markers',
                name='Predicci√≥n',
                line=dict(color='#1976d2', width=3),
                marker=dict(size=6)
            ))
            
            fig.update_layout(
                title=f"Predicci√≥n para Producto {prediction_data.get('producto_id', 'N/A')}",
                xaxis_title="D√≠as",
                yaxis_title="Demanda",
                height=400,
                template="plotly_white"
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
        except Exception as e:
            logger.warning(f"Error renderizando visualizaci√≥n: {e}")

    def _process_user_message(self, user_input: str) -> str:
        """Procesar mensaje del usuario y generar respuesta"""
        
        # Intentar inicializar Ollama si no est√° disponible
        if not self.ollama_client and OLLAMA_AVAILABLE:
            try:
                # Usar asyncio.run para la inicializaci√≥n
                asyncio.run(self._init_ollama_client())
            except Exception as e:
                logger.warning(f"No se pudo inicializar Ollama: {e}")
        
        # PRIMERO: Detectar si es una pregunta de seguimiento
        if self._detect_follow_up_question(user_input):
            logger.info(f"Pregunta de seguimiento detectada: {user_input}")
            return self._get_contextual_response(user_input)
        
        # SEGUNDO: Detectar intenci√≥n del usuario para nuevas consultas
        intent = self._detect_intent(user_input)
        
        logger.info(f"Intent detectado: {intent} para input: {user_input}")
        
        # Manejar seg√∫n la intenci√≥n
        if intent == 'prediction':
            return self._handle_prediction_request(user_input)
        elif intent == 'comparison':
            return self._handle_model_comparison(user_input)
        elif intent == 'analysis':
            return self._handle_analysis_request(user_input)
        else:  # general
            return self._handle_general_chat(user_input)
    
    def _detect_intent(self, user_input: str) -> str:
        """Detectar la intenci√≥n del usuario de manera m√°s inteligente"""
        user_input_lower = user_input.lower()
        
        # Palabras clave m√°s espec√≠ficas para cada intenci√≥n
        prediction_keywords = [
            'predecir', 'predicci√≥n', 'pron√≥stico', 'demanda', 'ventas futuras', 'futuro',
            'pr√≥ximos', 'd√≠as', 'semanas', 'meses', 'cu√°nto', 'cu√°ntos',
            'producto', 'cu√°l ser√°', 'cuanto voy a vender', 'proyecci√≥n',
            'estimaci√≥n', 'forecast', 'prever', 'proyectar'
        ]
        
        # Palabras espec√≠ficas para comparaci√≥n (m√°s restrictivas)
        comparison_keywords = [
            'comparar modelos', 'mejor modelo', 'qu√© modelo', 'cu√°l modelo',
            'modelo m√°s preciso', 'modelo m√°s exacto', 'accuracy', 'precisi√≥n del modelo',
            'performance modelo', 'rendimiento modelo', 'evaluar modelos',
            'algoritmo mejor', 'ml accuracy', 'que modelo es mejor', 'cual modelo es mejor',
            'modelo mejor', 'mejores modelos', 'compara modelos'
        ]
        
        # Palabras para an√°lisis de tendencias
        analysis_keywords = [
            'analizar tendencia', 'tendencia de ventas', 'patr√≥n', 'insight',
            'estudiar', 'examinar tendencia', 'revisar tendencia', 'investigar patr√≥n',
            'reportar tendencia', 'reporte', 'an√°lisis hist√≥rico', 'comportamiento',
            'categor√≠a', 'an√°lisis de categor√≠a'
        ]
        
        # Conversaci√≥n general
        general_keywords = [
            'hola', 'buenos', 'buenas', 'hey', 'hi', 'hello', 'qu√© haces',
            'que haces', 'gracias', 'thanks', 'ayuda', 'help', 'qu√© puedes',
            'capacidades', 'como funciona', 'c√≥mo funciona'
        ]
        
        # Detecci√≥n espec√≠fica por frases exactas (mayor prioridad)
        comparison_phrases = [
            'comparar modelos', 'qu√© modelo', 'cu√°l modelo', 'mejor modelo',
            'que modelo es mejor', 'cual modelo es mejor', 'modelo mejor',
            'mejores modelos', 'compara modelos'
        ]
        
        if any(phrase in user_input_lower for phrase in comparison_phrases):
            return 'comparison'
        
        if any(phrase in user_input_lower for phrase in ['analizar tendencia', 'tendencia de', 'an√°lisis de']):
            return 'analysis'
        
        # Buscar con n√∫meros de producto (indica predicci√≥n)
        import re
        if re.search(r'producto\s+\d+', user_input_lower) or re.search(r'\d+\s+d√≠as?', user_input_lower):
            return 'prediction'
        
        if re.search(r'demanda.*producto', user_input_lower) or re.search(r'cu√°l ser√°.*demanda', user_input_lower):
            return 'prediction'
        
        # Contar coincidencias por categor√≠a
        prediction_count = sum(1 for keyword in prediction_keywords if keyword in user_input_lower)
        comparison_count = sum(1 for keyword in comparison_keywords if keyword in user_input_lower)
        analysis_count = sum(1 for keyword in analysis_keywords if keyword in user_input_lower)
        general_count = sum(1 for keyword in general_keywords if keyword in user_input_lower)
        
        # Decidir basado en la mayor cantidad de coincidencias
        counts = {
            'prediction': prediction_count,
            'comparison': comparison_count,
            'analysis': analysis_count,
            'general': general_count
        }
        
        max_count = max(counts.values())
        if max_count == 0:
            return 'general'  # Si no hay coincidencias claras, asumir conversaci√≥n general
        
        # Retornar la intenci√≥n con m√°s coincidencias
        for intent, count in counts.items():
            if count == max_count:
                return intent
        
        return 'general'
    
    def _extract_product_id(self, user_input: str) -> int:
        """Extraer ID de producto del input del usuario"""
        import re
        
        # Buscar patrones como "producto 1", "producto X", etc.
        product_match = re.search(r'producto\s+(\d+)', user_input.lower())
        if product_match:
            return int(product_match.group(1))
        
        # Buscar n√∫meros directos en el texto
        number_match = re.search(r'\b(\d+)\b', user_input)
        if number_match:
            return int(number_match.group(1))
        
        # Si no encuentra un ID espec√≠fico, usar un ID por defecto basado en contexto
        user_lower = user_input.lower()
        if 'ropa' in user_lower or 'textil' in user_lower:
            return 1
        elif 'electronica' in user_lower or 'computadora' in user_lower:
            return 2
        elif 'comida' in user_lower or 'alimento' in user_lower:
            return 3
        elif 'hogar' in user_lower or 'casa' in user_lower:
            return 4
        elif 'deporte' in user_lower or 'fitness' in user_lower:
            return 5
        else:
            return 1  # ID por defecto

    def _save_tool_result(self, tool_type: str, result_data: Dict[str, Any], user_input: str):
        """Guardar resultados de herramientas para contexto futuro"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        if tool_type == 'prediction':
            prediction_summary = {
                'timestamp': timestamp,
                'user_query': user_input,
                'producto_id': result_data.get('producto_id', 'N/A'),
                'prediccion_promedio': result_data.get('predicciones', [0])[-1] if result_data.get('predicciones') else 0,
                'modelo_usado': result_data.get('mejor_modelo', 'N/A'),
                'confianza': result_data.get('confianza', 0),
                'tendencia': result_data.get('trend_type', 'N/A'),
                'raw_data': result_data
            }
            st.session_state.tool_results['recent_predictions'].append(prediction_summary)
            # Mantener solo las √∫ltimas 3
            if len(st.session_state.tool_results['recent_predictions']) > 3:
                st.session_state.tool_results['recent_predictions'].pop(0)
        
        elif tool_type == 'comparison':
            comparison_summary = {
                'timestamp': timestamp,
                'user_query': user_input,
                'mejor_modelo': result_data.get('best_model', 'Random Forest'),
                'r2_scores': result_data.get('r2_scores', {}),
                'conclusion': result_data.get('conclusion', 'Comparaci√≥n realizada'),
                'raw_data': result_data
            }
            st.session_state.tool_results['recent_comparisons'].append(comparison_summary)
            # Mantener solo las √∫ltimas 2
            if len(st.session_state.tool_results['recent_comparisons']) > 2:
                st.session_state.tool_results['recent_comparisons'].pop(0)
        
        elif tool_type == 'analysis':
            analysis_summary = {
                'timestamp': timestamp,
                'user_query': user_input,
                'tipo_analisis': result_data.get('analysis_type', 'general'),
                'categoria': result_data.get('category', 'N/A'),
                'insights': result_data.get('insights', []),
                'raw_data': result_data
            }
            st.session_state.tool_results['recent_analysis'].append(analysis_summary)
            # Mantener solo las √∫ltimas 2
            if len(st.session_state.tool_results['recent_analysis']) > 2:
                st.session_state.tool_results['recent_analysis'].pop(0)
        
        # Actualizar √∫ltimo resultado
        st.session_state.tool_results['last_action'] = tool_type
        st.session_state.tool_results['last_results'] = result_data

    def _detect_follow_up_question(self, user_input: str) -> bool:
        """Detectar si es una pregunta de seguimiento sobre resultados anteriores"""
        follow_up_patterns = [
            'qu√© puedes decir al respecto',
            'qu√© opinas sobre esto',
            'qu√© significa esto',
            'expl√≠came esto',
            'analiza esto',
            'interpreta esto',
            'qu√© conclusiones',
            'qu√© recomendaciones',
            'bas√°ndote en esto',
            'sobre estos resultados',
            'sobre esta informaci√≥n',
            'que me dices de',
            'c√≥mo interpretas',
            'qu√© piensas',
            'ejecuta el comando',
            'ejecuta eso',
            'hazlo',
            'ejecutalo',
            'realiza el comando',
            'corre el comando',
            'ejecuta para ver',
            'muestra el resultado',
            'dame los resultados'
        ]
        
        user_lower = user_input.lower()
        return any(pattern in user_lower for pattern in follow_up_patterns)

    def _get_contextual_response(self, user_input: str) -> str:
        """Generar respuesta contextual basada en resultados anteriores"""
        last_action = st.session_state.tool_results.get('last_action')
        last_results = st.session_state.tool_results.get('last_results')
        
        user_lower = user_input.lower()
        
        # Si el usuario pide ejecutar algo y no hay contexto previo, ejecutar herramienta apropiada
        if any(cmd in user_lower for cmd in ['ejecuta', 'ejecutalo', 'hazlo', 'muestra', 'realiza']):
            # Determinar qu√© ejecutar basado en el contexto de mensajes anteriores
            recent_messages = st.session_state.messages[-3:] if len(st.session_state.messages) >= 3 else st.session_state.messages
            
            for msg in reversed(recent_messages):
                if msg['role'] == 'user':
                    # Si el mensaje anterior mencionaba comparaci√≥n
                    if any(word in msg['content'].lower() for word in ['modelo', 'comparar', 'mejor', 'preciso']):
                        return self._handle_model_comparison(f"comparar modelos para {msg['content']}")
                    # Si mencionaba an√°lisis de categor√≠a
                    elif any(word in msg['content'].lower() for word in ['ropa', 'tendencia', 'espera', 'categoria']):
                        return self._handle_analysis_request(msg['content'])
                    # Si mencionaba predicci√≥n
                    elif any(word in msg['content'].lower() for word in ['predic', 'demanda', 'futuro', 'venta']):
                        return self._handle_prediction_request(msg['content'])
        
        # Si hay contexto previo, analizarlo
        if not last_action or not last_results:
            return self._get_intelligent_fallback(user_input)
        
        if last_action == 'comparison':
            return self._analyze_model_comparison_context(user_input, last_results)
        elif last_action == 'prediction':
            return self._analyze_prediction_context(user_input, last_results)
        elif last_action == 'analysis':
            return self._analyze_analysis_context(user_input, last_results)
        
        return self._get_intelligent_fallback(user_input)

    def _analyze_model_comparison_context(self, user_input: str, comparison_data: Dict) -> str:
        """Analizar y explicar resultados de comparaci√≥n de modelos"""
        try:
            # Obtener el √∫ltimo resultado de comparaci√≥n
            recent_comp = st.session_state.tool_results['recent_comparisons'][-1] if st.session_state.tool_results['recent_comparisons'] else None
            
            if not recent_comp:
                return "No encontr√© resultados de comparaci√≥n recientes para analizar."
            
            response = f"""
            <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #7b1fa2; color: #000000;">
            
            ## ü§î An√°lisis de la Comparaci√≥n de Modelos
            
            **Bas√°ndome en tu pregunta:** "{user_input}"
            
            ### üéØ Interpretaci√≥n de Resultados
            
            <div style="background-color: #e8f5e8; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #4caf50; color: #1b5e20; font-weight: 600;">
            üèÜ **Modelo Ganador:** {recent_comp['mejor_modelo']}
            </div>
            
            ### üìä ¬øQu√© significan estos n√∫meros?
            
            **R¬≤ Score (Coeficiente de Determinaci√≥n):**
            - üìà Mide qu√© tan bien el modelo explica la variabilidad de tus datos
            - üéØ Rango: 0.0 (terrible) a 1.0 (perfecto)
            - ‚úÖ **Mayor R¬≤ = Mejor predicci√≥n**
            
            **MSE (Error Cuadr√°tico Medio):**
            - üìâ Promedio de errores al cuadrado
            - üéØ **Menor MSE = Mejor precisi√≥n**
            
            **MAE (Error Absoluto Medio):**
            - üìä Error promedio sin elevar al cuadrado
            - üéØ **Menor MAE = Predicciones m√°s cercanas**
            
            ### üí° Recomendaciones Pr√°cticas
            
            <div style="background-color: #f3e5f5; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #9c27b0; color: #4a148c; font-weight: 600;">
            üöÄ **Para tu negocio:** El {recent_comp['mejor_modelo']} te dar√° las predicciones m√°s confiables
            </div>
            
            **Pr√≥ximos pasos sugeridos:**
            1. üìä Usa el {recent_comp['mejor_modelo']} para predicciones futuras
            2. üîÑ Re-eval√∫a mensualmente con nuevos datos
            3. üìà Monitora la precisi√≥n en la pr√°ctica
            
            ### ü§ñ ¬øQuieres que realice alg√∫n an√°lisis espec√≠fico?
            
            Puedo ayudarte con:
            - üìä Predicci√≥n de demanda usando el mejor modelo
            - üìà An√°lisis de tendencias espec√≠ficas
            - üéØ Recomendaciones personalizadas para tu inventario
            
            </div>
            """
            
            return response
            
        except Exception as e:
            return f"Error analizando la comparaci√≥n: {str(e)}"

    def _analyze_prediction_context(self, user_input: str, prediction_data: Dict) -> str:
        """Analizar y explicar resultados de predicci√≥n"""
        try:
            recent_pred = st.session_state.tool_results['recent_predictions'][-1] if st.session_state.tool_results['recent_predictions'] else None
            
            if not recent_pred:
                return "No encontr√© predicciones recientes para analizar."
            
            response = f"""
            <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #1976d2; color: #000000;">
            
            ## üìä An√°lisis de tu Predicci√≥n de Demanda
            
            **Respondiendo a:** "{user_input}"
            
            ### üéØ Resumen de Resultados
            
            <div style="background-color: #e3f2fd; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #1976d2; color: #0d47a1; font-weight: 600;">
            üì¶ **Producto {recent_pred['producto_id']}:** {recent_pred['prediccion_promedio']:.1f} unidades promedio
            </div>
            
            ### üîç Interpretaci√≥n Detallada
            
            **Confianza del Modelo:**
            - üéØ **{recent_pred['confianza']:.1%}** de confianza en la predicci√≥n
            - ü§ñ Modelo usado: **{recent_pred['modelo_usado']}**
            - üìà Tendencia detectada: **{recent_pred['tendencia']}**
            
            ### üí° ¬øQu√© significa para tu negocio?
            
            **Planificaci√≥n de Inventario:**
            """
            
            # Agregar recomendaciones basadas en la predicci√≥n
            prediccion = recent_pred['prediccion_promedio']
            if prediccion > 100:
                response += """
            <div style="background-color: #fff3e0; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #ff9800; color: #e65100; font-weight: 600;">
            üî• **Alta demanda proyectada** - Considera aumentar tu inventario
            </div>
            """
            elif prediccion > 50:
                response += """
            <div style="background-color: #e8f5e8; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #4caf50; color: #1b5e20; font-weight: 600;">
            ‚úÖ **Demanda moderada** - Mant√©n niveles de stock normales
            </div>
            """
            else:
                response += """
            <div style="background-color: #fce4ec; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #e91e63; color: #880e4f; font-weight: 600;">
            üìâ **Demanda baja** - Eval√∫a promociones o reducir inventario
            </div>
            """
            
            response += f"""
            ### üìã Acciones Recomendadas
            
            1. üìä **Stock √≥ptimo:** {prediccion * 1.2:.0f} unidades (20% buffer)
            2. üîÑ **Punto de reorden:** {prediccion * 0.3:.0f} unidades
            3. üìÖ **Pr√≥xima revisi√≥n:** En 1 semana
            
            ### ü§ñ ¬øTe ayudo con algo m√°s?
            
            Puedo ayudarte a:
            - üîç Comparar con otros productos
            - üìà Analizar tendencias hist√≥ricas
            - üí∞ Calcular rentabilidad proyectada
            
            </div>
            """
            
            return response
            
        except Exception as e:
            return f"Error analizando la predicci√≥n: {str(e)}"

    def _analyze_analysis_context(self, user_input: str, analysis_data: Dict) -> str:
        """Analizar y explicar resultados de an√°lisis general"""
        try:
            recent_analysis = st.session_state.tool_results['recent_analysis'][-1] if st.session_state.tool_results['recent_analysis'] else None
            
            if not recent_analysis:
                return "No encontr√© an√°lisis recientes para interpretar."
            
            response = f"""
            <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #ff9800; color: #000000;">
            
            ## üìà Interpretaci√≥n del An√°lisis
            
            **Tu consulta:** "{user_input}"
            
            ### üéØ Resumen del An√°lisis Realizado
            
            <div style="background-color: #fff3e0; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #ff9800; color: #e65100; font-weight: 600;">
            üìä **Tipo:** {recent_analysis['tipo_analisis']} | **Categor√≠a:** {recent_analysis['categoria']}
            </div>
            
            ### üí° Insights Clave
            """
            
            for insight in recent_analysis.get('insights', [])[:3]:  # Mostrar m√°ximo 3
                response += f"""
            - üéØ {insight}
            """
            
            response += """
            
            ### üöÄ Pr√≥ximos Pasos Sugeridos
            
            1. üìä Monitorear tendencias identificadas
            2. üîÑ Ajustar estrategias seg√∫n insights
            3. üìà Revisar resultados en 2 semanas
            
            ### ü§ñ ¬øNecesitas m√°s detalles?
            
            Puedo profundizar en:
            - üìä An√°lisis espec√≠ficos por producto
            - üîç Comparaciones detalladas
            - üí° Recomendaciones personalizadas
            
            </div>
            """
            
            return response
            
        except Exception as e:
            return f"Error analizando el an√°lisis: {str(e)}"

    def _handle_prediction_request(self, user_input: str) -> str:
        """Manejar solicitud de predicci√≥n"""
        try:
            # Extraer par√°metros de la sesi√≥n
            product_id = self._extract_product_id(user_input)
            
            # Crear petici√≥n (formato compatible con backend)
            request_data = {
                "producto_id": product_id,
                "dias_adelante": st.session_state.get('prediction_days', 30),
                "include_confidence": True,
                "use_cache": True
            }
            
            # Intentar llamar al backend
            try:
                response = requests.post(
                    f"{self.backend_url}/api/predict/demanda",
                    json=request_data,
                    timeout=5  # Timeout corto para demo
                )
                
                if response.status_code == 200:
                    prediction_data = response.json()
                    
                    # Guardar en historial
                    st.session_state.prediction_history.append({
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
                        "confidence": prediction_data.get('confianza', 0),
                        "model": prediction_data.get('mejor_modelo', 'unknown'),
                        "data": prediction_data
                    })
                    
                    # Guardar resultado para contexto futuro
                    self._save_tool_result('prediction', prediction_data, user_input)
                    
                    # Generar respuesta interpretativa
                    return self._generate_prediction_interpretation(prediction_data, user_input)
                else:
                    # Backend responde pero hay error, usar datos simulados
                    return self._generate_demo_prediction(product_id, user_input)
                    
            except (requests.RequestException, requests.Timeout):
                # Backend no disponible, usar datos simulados
                return self._generate_demo_prediction(product_id, user_input)
                
        except Exception as e:
            return f"Error procesando predicci√≥n: {str(e)}"
    
    def _generate_demo_prediction(self, product_id: int, user_input: str) -> str:
        """Generar predicci√≥n de demostraci√≥n cuando el backend no est√° disponible"""
        import numpy as np
        import hashlib
        
        # Usar hash del input para generar datos √∫nicos pero consistentes
        seed_value = int(hashlib.md5(f"{product_id}_{user_input}".encode()).hexdigest()[:8], 16) % 10000
        np.random.seed(seed_value)
        
        days = st.session_state.get('prediction_days', 30)
        
        # Crear predicci√≥n simulada m√°s variada
        base_demand = 30 + (product_id % 20) * 5 + np.random.randint(-10, 20)
        
        # Diferentes tipos de tendencia basados en el producto
        trend_types = ['creciente', 'decreciente', 'estacional', 'estable']
        trend_type = trend_types[product_id % 4]
        
        predicted_values = []
        for i in range(days):
            if trend_type == 'creciente':
                trend_value = base_demand + (i * 0.5) + np.random.normal(0, 2)
            elif trend_type == 'decreciente':
                trend_value = base_demand - (i * 0.3) + np.random.normal(0, 2)
            elif trend_type == 'estacional':
                # Patr√≥n semanal
                seasonal = 15 * np.sin(2 * np.pi * i / 7) + 5 * np.cos(2 * np.pi * i / 14)
                trend_value = base_demand + seasonal + np.random.normal(0, 3)
            else:  # estable
                trend_value = base_demand + np.random.normal(0, 4)
            
            value = max(1, int(trend_value))
            predicted_values.append(value)
        
        # Calcular m√©tricas m√°s realistas
        avg_demand = np.mean(predicted_values)
        trend = trend_type
        
        # Confidence basada en tipo de tendencia
        confidence_base = {
            'estable': 0.85,
            'creciente': 0.78,
            'decreciente': 0.72,
            'estacional': 0.68
        }
        
        confidence = confidence_base[trend_type] + np.random.uniform(-0.08, 0.08)
        confidence = max(0.6, min(0.95, confidence))
        
        # Seleccionar modelo basado en tendencia
        model_selection = {
            'estable': 'linear',
            'creciente': 'linear', 
            'decreciente': 'polynomial',
            'estacional': 'random_forest'
        }
        
        selected_model = model_selection[trend_type]
        
        prediction_data = {
            'predicciones': predicted_values,
            'confianza': confidence,
            'mejor_modelo': selected_model,
            'dias_adelante': days,
            'producto_id': product_id,
            'trend_type': trend_type
        }
        
        # Guardar resultados en el contexto de herramientas
        self._save_tool_result('prediction', prediction_data, user_input)
        
        # Guardar en historial
        st.session_state.prediction_history.append({
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
            "confidence": confidence,
            "model": selected_model,
            "trend": trend_type,
            "data": prediction_data
        })
        
        # Agregar nota de demostraci√≥n
        demo_note = "üìç **Modo Demo**: Datos simulados (backend en desarrollo)"
        interpretation = self._generate_prediction_interpretation(prediction_data, user_input)
        
        return f"{demo_note}\n\n{interpretation}"

    def _generate_prediction_interpretation(self, prediction_data: Dict[str, Any], user_input: str) -> str:
        """Generar interpretaci√≥n detallada de la predicci√≥n"""
        try:
            product_id = prediction_data.get('producto_id', 'N/A')
            predictions = prediction_data.get('predicciones', [])
            confidence = prediction_data.get('confianza', 0)
            model = prediction_data.get('mejor_modelo', 'unknown')
            trend_type = prediction_data.get('trend_type', 'unknown')
            days = prediction_data.get('dias_adelante', 30)
            
            if not predictions:
                return "No se pudieron generar predicciones para este producto."
            
            avg_demand = sum(predictions) / len(predictions)
            max_demand = max(predictions)
            min_demand = min(predictions)
            
            # Determinar tendencia visual
            if len(predictions) >= 7:
                early_avg = sum(predictions[:7]) / 7
                late_avg = sum(predictions[-7:]) / 7
                trend_direction = "creciente" if late_avg > early_avg * 1.1 else "decreciente" if late_avg < early_avg * 0.9 else "estable"
            else:
                trend_direction = trend_type
            
            response = f"""
            <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #1976d2; color: #000000;">
            
            ## üìä Predicci√≥n de Demanda - Producto {product_id}
            
            **Consulta:** "{user_input}"
            
            ### üéØ Resumen Ejecutivo
            
            <div style="background-color: #e3f2fd; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #1976d2; color: #0d47a1; font-weight: 600;">
            üì¶ **Demanda promedio**: {avg_demand:.1f} unidades diarias
            </div>
            
            ### üìà M√©tricas Clave
            
            - üéØ **Confianza del modelo**: {confidence:.1%}
            - ü§ñ **Modelo utilizado**: {model.title()}
            - üìä **Per√≠odo analizado**: {days} d√≠as
            - üìà **Tendencia**: {trend_direction.title()}
            - üìä **Rango**: {min_demand:.0f} - {max_demand:.0f} unidades
            
            ### üí° Interpretaci√≥n de Resultados
            
            """
            
            # Recomendaciones basadas en la demanda promedio
            if avg_demand > 100:
                response += """
            <div style="background-color: #fff3e0; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #ff9800; color: #e65100; font-weight: 600;">
            üî• **Alta Demanda Proyectada** - Este producto muestra excelente potencial de ventas
            </div>
            
            **Estrategia recomendada:**
            - üìà **Aumentar inventario** inmediatamente
            - üéØ **Stock objetivo**: {int(avg_demand * 1.5):.0f} unidades diarias
            - üîÑ **Punto de reorden**: {int(avg_demand * 0.7):.0f} unidades
            """
            elif avg_demand > 50:
                response += """
            <div style="background-color: #e8f5e8; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #4caf50; color: #1b5e20; font-weight: 600;">
            ‚úÖ **Demanda Moderada** - Mant√©n niveles de stock normales
            </div>
            
            **Estrategia recomendada:**
            - üìä **Mantener niveles actuales** de inventario
            - üéØ **Stock objetivo**: {int(avg_demand * 1.3):.0f} unidades diarias
            - üîÑ **Punto de reorden**: {int(avg_demand * 0.5):.0f} unidades
            """
            else:
                response += """
            <div style="background-color: #fce4ec; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #e91e63; color: #880e4f; font-weight: 600;">
            üìâ **Demanda Baja** - Eval√∫a promociones o reducir inventario
            </div>
            
            **Estrategia recomendada:**
            - üéØ **Evaluar promociones** o descuentos
            - üì¶ **Reducir inventario** gradualmente
            - üîç **Investigar** factores que afectan la demanda
            """
            
            # Recomendaciones basadas en tendencia
            if trend_direction == "creciente":
                response += """
            - üìà **Tendencia creciente**: Prep√°rate para mayor demanda
            - üöÄ **Oportunidad**: Considera expandir la l√≠nea de productos similares
            """
            elif trend_direction == "decreciente":
                response += """
            - üìâ **Tendencia decreciente**: Ajusta estrategias de marketing
            - üîÑ **Acci√≥n**: Eval√∫a factores estacionales o competencia
            """
            else:
                response += """
            - üìä **Tendencia estable**: Demanda predecible y confiable
            - ‚úÖ **Ventaja**: F√°cil planificaci√≥n de inventario
            """
            
            response += f"""
            
            ### üìã Plan de Acci√≥n Recomendado
            
            1. **üìä Monitoreo semanal** de las ventas reales vs predicci√≥n
            2. **üîÑ Ajuste de inventario** seg√∫n la tendencia observada
            3. **üìÖ Revisi√≥n en 2 semanas** para evaluar precisi√≥n del modelo
            4. **üéØ Optimizaci√≥n** basada en los resultados obtenidos
            
            ### ü§ñ ¬øNecesitas m√°s an√°lisis?
            
            Puedo ayudarte con:
            - üîç **Comparar con otros productos** de tu inventario
            - üìà **Analizar tendencias** por categor√≠a
            - üí∞ **Calcular rentabilidad** proyectada
            
            </div>
            """
            
            return response
            
        except Exception as e:
            return f"Error generando interpretaci√≥n de predicci√≥n: {str(e)}"

    def _generate_comparison_interpretation(self, comparison_data: Dict[str, Any], user_input: str) -> str:
        """Generar interpretaci√≥n detallada de la comparaci√≥n de modelos"""
        try:
            response = f"""
            <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #7b1fa2; color: #000000;">
            
            ## üîç An√°lisis Detallado de Comparaci√≥n de Modelos
            
            **Consulta:** "{user_input}"
            
            ### üèÜ Resultado de la Comparaci√≥n
            
            Bas√°ndome en los datos de rendimiento, he evaluado los siguientes modelos de Machine Learning:
            
            """
            
            # Si hay datos espec√≠ficos de comparaci√≥n, usarlos
            if comparison_data and 'models' in comparison_data:
                for model_name, metrics in comparison_data['models'].items():
                    response += f"""
            **ü§ñ {model_name.title()}:**
            - R¬≤ Score: {metrics.get('r2', 0):.3f}
            - MSE: {metrics.get('mse', 0):.3f}  
            - MAE: {metrics.get('mae', 0):.3f}
            """
            
            response += """
            
            ### üìä ¬øQu√© significan estas m√©tricas?
            
            **R¬≤ Score (Coeficiente de Determinaci√≥n):**
            - üìà Mide qu√© tan bien explica el modelo la variabilidad de tus datos
            - üéØ Rango: 0.0 (malo) a 1.0 (perfecto)
            - ‚úÖ **Mayor R¬≤ = Mejor capacidad predictiva**
            
            **MSE (Error Cuadr√°tico Medio):**
            - üìâ Penaliza m√°s los errores grandes
            - üéØ **Menor MSE = Mayor precisi√≥n**
            
            **MAE (Error Absoluto Medio):**
            - üìä Error promedio en t√©rminos absolutos
            - üéØ **Menor MAE = Predicciones m√°s cercanas**
            
            ### üí° Recomendaci√≥n para tu Negocio
            
            <div style="background-color: #f3e5f5; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #9c27b0; color: #4a148c; font-weight: 600;">
            üöÄ **Usa el modelo con mayor R¬≤** para tus predicciones futuras
            </div>
            
            ### üéØ Gu√≠a de Selecci√≥n por Caso de Uso
            
            - **Linear**: ‚ö° Ideal para tendencias simples y respuesta r√°pida
            - **Polynomial**: üîÑ Mejor para patrones con curvas y estacionalidad
            - **Random Forest**: üéØ M√°xima precisi√≥n para datos complejos
            
            ### üìã Pr√≥ximos Pasos Sugeridos
            
            1. **üìä Implementar** el modelo recomendado en tus predicciones
            2. **üîÑ Monitorear** la precisi√≥n en datos reales durante 2 semanas
            3. **üìà Re-evaluar** mensualmente con datos nuevos
            4. **üéØ Ajustar** si la precisi√≥n baja del 80%
            
            </div>
            """
            
            return response
            
        except Exception as e:
            return f"Error generando interpretaci√≥n de comparaci√≥n: {str(e)}"

    def _handle_model_comparison(self, user_input: str) -> str:
        """Manejar comparaci√≥n de modelos con fallback inteligente"""
        try:
            # Intentar obtener comparaci√≥n del backend
            comparison_id = f"comp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            response = requests.get(
                f"{self.backend_url}/api/predict/models/comparison/{comparison_id}",
                timeout=5  # Timeout corto
            )
            
            if response.status_code == 200:
                comparison_data = response.json()
                
                # Guardar resultado para contexto futuro
                self._save_tool_result('comparison', comparison_data, user_input)
                
                return self._generate_comparison_interpretation(comparison_data, user_input)
            else:
                # Si falla el backend, generar comparaci√≥n simulada
                return self._generate_demo_comparison(user_input)
                
        except Exception as e:
            logger.warning(f"Error en comparaci√≥n del backend: {e}")
            return self._generate_demo_comparison(user_input)

    def _generate_demo_comparison(self, user_input: str) -> str:
        """Generar comparaci√≥n de modelos simulada"""
        import numpy as np
        
        # Datos simulados de comparaci√≥n
        models_comparison = {
            'linear': {
                'mse': np.random.uniform(0.15, 0.25),
                'r2': np.random.uniform(0.75, 0.85),
                'mae': np.random.uniform(0.12, 0.18),
                'speed': 'Muy r√°pido',
                'complexity': 'Baja'
            },
            'polynomial': {
                'mse': np.random.uniform(0.10, 0.20),
                'r2': np.random.uniform(0.80, 0.90),
                'mae': np.random.uniform(0.08, 0.15),
                'speed': 'Moderado',
                'complexity': 'Media'
            },
            'random_forest': {
                'mse': np.random.uniform(0.08, 0.15),
                'r2': np.random.uniform(0.85, 0.95),
                'mae': np.random.uniform(0.06, 0.12),
                'speed': 'Lento',
                'complexity': 'Alta'
            }
        }
        
        # Encontrar el mejor modelo por R¬≤
        best_model = max(models_comparison.keys(), key=lambda k: models_comparison[k]['r2'])
        
        # Guardar resultados en el contexto
        comparison_result = {
            'best_model': best_model.replace('_', ' ').title(),
            'r2_scores': {k: v['r2'] for k, v in models_comparison.items()},
            'models_data': models_comparison,
            'conclusion': f"{best_model.replace('_', ' ').title()} es el m√°s preciso"
        }
        self._save_tool_result('comparison', comparison_result, user_input)
        
        interpretation = f"""
        <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #e0e0e0; color: #000000;">
        
        ## üîç Comparaci√≥n de Modelos de ML
        
        **Bas√°ndome en tu consulta:** "{user_input}"
        
        ### üèÜ Modelo Recomendado: **{best_model.replace('_', ' ').title()}**
        
        ### üìä Resultados Detallados
        
        """
        
        for model, metrics in models_comparison.items():
            model_name = model.replace('_', ' ').title()
            r2_color = "#1b5e20" if metrics['r2'] > 0.85 else "#f57c00" if metrics['r2'] > 0.75 else "#d32f2f"
            
            interpretation += f"""
        **ü§ñ {model_name}:**
        - **R¬≤ Score**: <span style="color: {r2_color}; font-weight: bold; background-color: #f5f5f5; padding: 2px 6px; border-radius: 4px;">{metrics['r2']:.3f}</span>
        - **MSE**: <span style="color: #000000; font-weight: bold;">{metrics['mse']:.3f}</span>
        - **MAE**: <span style="color: #000000; font-weight: bold;">{metrics['mae']:.3f}</span>
        - **Velocidad**: <span style="color: #4a148c; font-weight: bold;">{metrics['speed']}</span>
        - **Complejidad**: <span style="color: #4a148c; font-weight: bold;">{metrics['complexity']}</span>
        
        """
        
        interpretation += f"""
        ### üí° Recomendaciones
        
        <div style="background-color: #e8f5e8; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #4caf50; color: #1b5e20; font-weight: 600;">
        üéØ **Mejor opci√≥n:** {best_model.replace('_', ' ').title()} con R¬≤ de {models_comparison[best_model]['r2']:.3f}
        </div>
        
        <div style="background-color: #f3e5f5; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #9c27b0; color: #4a148c; font-weight: 600;">
        üìà **Criterio de selecci√≥n:** Mayor R¬≤ indica mejor capacidad predictiva
        </div>
        
        ### üìã Gu√≠a de Selecci√≥n:
        - **Linear**: Ideal para tendencias simples y predicciones r√°pidas
        - **Polynomial**: Mejor para patrones m√°s complejos
        - **Random Forest**: M√°xima precisi√≥n para datos complejos
        
        </div>
        """
        
        return interpretation
    
    def _handle_analysis_request(self, user_input: str) -> str:
        """Manejar solicitud de an√°lisis con capacidades mejoradas"""
        
        # Extraer informaci√≥n de la consulta
        category = self._extract_category_from_input(user_input)
        
        if category:
            return self._generate_category_analysis(category, user_input)
        else:
            return self._generate_general_analysis(user_input)
    
    def _extract_category_from_input(self, user_input: str) -> str:
        """Extraer categor√≠a de producto del input del usuario"""
        user_input_lower = user_input.lower()
        
        categories = {
            'electronica': ['electronica', 'electronics', 'computadora', 'telefono', 'gadget'],
            'ropa': ['ropa', 'clothing', 'vestimenta', 'textil', 'fashion'],
            'comida': ['comida', 'food', 'alimento', 'bebida', 'restaurant'],
            'hogar': ['hogar', 'casa', 'home', 'domestico'],
            'deportes': ['deporte', 'sport', 'fitness', 'ejercicio'],
            'salud': ['salud', 'health', 'medicina', 'farmacia'],
            'belleza': ['belleza', 'beauty', 'cosm√©tico', 'cuidado personal']
        }
        
        for category, keywords in categories.items():
            if any(keyword in user_input_lower for keyword in keywords):
                return category
        
        return 'general'

    def _generate_category_analysis(self, category: str, user_input: str) -> str:
        """Generar an√°lisis espec√≠fico por categor√≠a"""
        import numpy as np
        
        # Generar datos simulados para la categor√≠a
        np.random.seed(hash(category) % 1000)
        
        # Simular tendencias de los √∫ltimos 6 meses
        months = ['Enero', 'Febrero', 'Marzo', 'Abril', 'Mayo', 'Junio']
        base_sales = 1000 + hash(category) % 500
        
        monthly_data = []
        for i, month in enumerate(months):
            # Tendencia creciente con variaci√≥n
            trend_factor = 1 + (i * 0.1) + np.random.uniform(-0.05, 0.05)
            sales = int(base_sales * trend_factor)
            monthly_data.append({'month': month, 'sales': sales})
        
        # Calcular insights
        total_sales = sum(data['sales'] for data in monthly_data)
        avg_growth = (monthly_data[-1]['sales'] - monthly_data[0]['sales']) / monthly_data[0]['sales'] * 100
        
        response = f"""
        <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #ff9800; color: #000000;">
        
        ## üìà An√°lisis de Tendencias - Categor√≠a: {category.title()}
        
        **üìç Modo Demo**: Datos simulados para demostraci√≥n
        
        ### üìä Resumen Ejecutivo
        
        <div style="background-color: #fff3e0; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #ff9800; color: #e65100; font-weight: 600;">
        üìà **Crecimiento promedio:** {avg_growth:.1f}% en los √∫ltimos 6 meses
        </div>
        
        ### üìÖ Tendencias Mensuales Recientes
        
        """
        
        for data in monthly_data[-3:]:  # √öltimos 3 meses
            response += f"- **{data['month']}**: {data['sales']:,} unidades vendidas\n"
        
        response += f"""
        
        ### üí° Insights Clave para {category.title()}
        
        - üéØ **Demanda estacional**: {category.title()} muestra patrones predecibles
        - üìä **Volumen total**: {total_sales:,} unidades en 6 meses  
        - üîÑ **Estrategia**: {"üìà Aumentar inventario - tendencia creciente" if avg_growth > 5 else "üìä Mantener niveles actuales - demanda estable"}
        - üí∞ **Oportunidad**: {"Alto potencial de crecimiento" if avg_growth > 10 else "Mercado estable y confiable"}
        
        ### üöÄ Recomendaciones Espec√≠ficas
        
        1. **üìä Stock √≥ptimo**: {int(monthly_data[-1]['sales'] * 1.3):,} unidades (30% buffer)
        2. **üéØ Punto de reorden**: {int(monthly_data[-1]['sales'] * 0.4):,} unidades  
        3. **üìÖ Pr√≥xima revisi√≥n**: En 2 semanas
        4. **ÔøΩ Monitoreo**: Tendencias semanales por subcategor√≠a
        
        ### ü§ñ ¬øNecesitas algo m√°s espec√≠fico?
        
        Puedo ayudarte con:
        - üìä **Predicci√≥n espec√≠fica**: "Predice la demanda para el producto X"
        - ÔøΩ **Comparar modelos**: "¬øQu√© modelo es m√°s preciso?"
        - üìà **An√°lisis detallado**: "Analiza otra categor√≠a"
        
        </div>
        """
        
        # Guardar en contexto
        analysis_data = {
            'analysis_type': 'category_trends',
            'category': category,
            'insights': [
                f"Crecimiento del {avg_growth:.1f}% en 6 meses",
                f"Volumen total: {total_sales:,} unidades",
                "Patrones estacionales detectados"
            ]
        }
        self._save_tool_result('analysis', analysis_data, user_input)
        
        return response

    def _generate_general_analysis(self, user_input: str) -> str:
        """Generar an√°lisis general cuando no se detecta categor√≠a espec√≠fica"""
        
        response = f"""
        <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #9c27b0; color: #000000;">
        
        ## üîç An√°lisis General de Tendencias
        
        **Tu consulta:** "{user_input}"
        
        ### üìä Insights Disponibles
        
        Para brindarte un an√°lisis m√°s espec√≠fico, puedo ayudarte con:
        
        **Por Categor√≠a:**
        - üëï Ropa y Fashion
        - üíª Electr√≥nicos
        - üçï Alimentos y Bebidas
        - üè† Hogar y Decoraci√≥n
        - üí™ Deportes y Fitness
        
        **Por Producto:**
        - üìä Predicciones espec√≠ficas
        - üìà An√°lisis de tendencias hist√≥ricas
        - üîç Comparaci√≥n con competencia
        
        ### üí° Recomendaci√≥n
        
        <div style="background-color: #f3e5f5; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #9c27b0; color: #4a148c; font-weight: 600;">
        üéØ **Especifica una categor√≠a** para obtener insights m√°s detallados
        </div>
        
        **Ejemplos de consultas m√°s espec√≠ficas:**
        - "Analiza las tendencias de ropa"
        - "¬øC√≥mo est√° el mercado de electr√≥nicos?"
        - "Tendencias en comida r√°pida"
        
        </div>
        """
        
        return response

    def _extract_category_from_input(self, user_input: str) -> str:
        """Extraer categor√≠a del input del usuario"""
        user_input_lower = user_input.lower()
        
        categories = {
            'electr√≥nicos': ['electronico', 'electronica', 'electronic', 'tecnologia'],
            'ropa': ['ropa', 'vestimenta', 'clothing', 'textil'],
            'alimentaci√≥n': ['alimento', 'comida', 'food', 'bebida'],
            'hogar': ['hogar', 'casa', 'home', 'domestico'],
            'deportes': ['deporte', 'sport', 'fitness', 'ejercicio'],
            'salud': ['salud', 'health', 'medicina', 'farmacia'],
            'belleza': ['belleza', 'beauty', 'cosmetico', 'maquillaje']
        }
        
        for category, keywords in categories.items():
            if any(keyword in user_input_lower for keyword in keywords):
                return category
        
        return None

    def _generate_category_analysis