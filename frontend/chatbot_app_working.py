"""
Frontend para Ollama - Interfaz de Chat para Predicci√≥n de Demanda
================================================================

Aplicaci√≥n Streamlit que proporciona una interfaz de usuario moderna
para interactuar con el sistema de predicci√≥n de demanda a trav√©s de Ollama.
"""

import streamlit as st
import asyncio
import json
import requests
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import sys
import os

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Agregar el directorio padre al path para importar m√≥dulos
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Importaciones b√°sicas sin Ollama por ahora
try:
    from chatbot.communication_schema import (
        PredictionScope, ModelType, RequestType
    )
    OLLAMA_AVAILABLE = True
except ImportError as e:
    print(f"Ollama integration not available: {e}")
    OLLAMA_AVAILABLE = False
    # Definir enums b√°sicos como fallback
    class PredictionScope:
        SINGLE_PRODUCT = "single_product"
        CATEGORY = "category" 
        BUSINESS = "business"
        MARKET = "market"
    
    class ModelType:
        AUTO_SELECT = "auto_select"
        LINEAR = "linear"
        POLYNOMIAL = "polynomial"


# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="MicroAnalytics - Chat de Predicci√≥n",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS optimizado para m√°xima legibilidad y contraste
st.markdown("""
<style>
/* Reset y base */
.main {
    padding-top: 1rem;
}

/* Estilos base para mensajes del chat con contraste m√°ximo */
.stChatMessage {
    border-radius: 12px;
    padding: 20px;
    margin: 15px 0;
    font-size: 16px;
    line-height: 1.7;
    border: 2px solid transparent;
}

.user-message {
    background-color: #ffffff !important;
    border: 3px solid #1976d2 !important;
    color: #0d47a1 !important;
    font-weight: 600;
    box-shadow: 0 3px 10px rgba(25, 118, 210, 0.2);
}

.user-message strong {
    color: #0d47a1 !important;
    font-weight: 700;
}

.assistant-message {
    background-color: #ffffff !important;
    border: 3px solid #7b1fa2 !important;
    color: #212121 !important;
    box-shadow: 0 4px 15px rgba(123, 31, 162, 0.15);
}

/* Forzar color negro en todos los elementos del asistente */
.assistant-message,
.assistant-message *,
.assistant-message p,
.assistant-message div,
.assistant-message span,
.assistant-message li,
.assistant-message td,
.assistant-message th {
    color: #000000 !important;
    font-weight: 500 !important;
}

/* T√≠tulos con contraste extremo */
.assistant-message h1,
.assistant-message h2,
.assistant-message h3,
.assistant-message h4,
.assistant-message h5,
.assistant-message h6 {
    color: #000000 !important;
    font-weight: 800 !important;
    margin: 20px 0 15px 0 !important;
    text-shadow: none !important;
    background-color: #e8eaf6 !important;
    padding: 8px 12px !important;
    border-radius: 6px !important;
    border-left: 4px solid #3f51b5 !important;
}

/* Texto enfatizado negro s√≥lido */
.assistant-message strong,
.assistant-message b {
    color: #000000 !important;
    font-weight: 800 !important;
    background-color: #fff3e0 !important;
    padding: 2px 4px !important;
    border-radius: 3px !important;
}

/* Enlaces completamente visibles */
.assistant-message a {
    color: #000000 !important;
    text-decoration: underline !important;
    font-weight: 700 !important;
    background-color: #e3f2fd !important;
    padding: 2px 4px !important;
    border-radius: 3px !important;
}

/* Indicadores de confianza con contraste extremo */
.confidence-high { 
    color: #000000 !important; 
    font-weight: 900 !important;
    background-color: #c8e6c9 !important;
    padding: 6px 12px !important;
    border-radius: 6px !important;
    border: 3px solid #2e7d32 !important;
    text-shadow: none !important;
}

.confidence-medium { 
    color: #000000 !important; 
    font-weight: 900 !important;
    background-color: #ffe0b2 !important;
    padding: 6px 12px !important;
    border-radius: 6px !important;
    border: 3px solid #f57c00 !important;
    text-shadow: none !important;
}

.confidence-low { 
    color: #000000 !important; 
    font-weight: 900 !important;
    background-color: #ffcdd2 !important;
    padding: 6px 12px !important;
    border-radius: 6px !important;
    border: 3px solid #d32f2f !important;
    text-shadow: none !important;
}

/* Listas completamente negras */
.assistant-message ul,
.assistant-message ol {
    color: #000000 !important;
    margin: 15px 0 !important;
}

.assistant-message ul li,
.assistant-message ol li {
    color: #000000 !important;
    font-weight: 600 !important;
    margin: 8px 0 !important;
    padding-left: 10px !important;
}

.assistant-message ul li::marker,
.assistant-message ol li::marker {
    color: #000000 !important;
}

/* C√≥digo completamente visible */
.assistant-message code,
.assistant-message pre {
    background-color: #f5f5f5 !important;
    color: #000000 !important;
    border: 2px solid #666666 !important;
    border-radius: 4px !important;
    padding: 8px !important;
    font-weight: 600 !important;
}

/* Forzar estilos en todos los elementos de Streamlit */
div[data-testid="stMarkdown"],
div[data-testid="stMarkdown"] *,
.stMarkdown,
.stMarkdown *,
.st-emotion-cache-acwcvw,
.st-emotion-cache-acwcvw *,
.st-emotion-cache-1sdpuyj,
.st-emotion-cache-1sdpuyj * {
    color: #000000 !important;
    font-weight: 500 !important;
}

/* Selectores espec√≠ficos para elementos problem√°ticos */
.assistant-message div[data-testid="stMarkdown"] p,
.assistant-message div[data-testid="stMarkdown"] div,
.assistant-message div[data-testid="stMarkdown"] span,
.assistant-message div[data-testid="stMarkdown"] li,
.assistant-message .stMarkdown p,
.assistant-message .stMarkdown div,
.assistant-message .stMarkdown span,
.assistant-message .stMarkdown li {
    color: #000000 !important;
    font-weight: 500 !important;
}

/* Sidebar mejorado */
.sidebar-section {
    margin-bottom: 30px;
    padding: 20px;
    background-color: #ffffff;
    border-radius: 10px;
    border: 2px solid #e0e0e0;
    color: #000000 !important;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
}

.sidebar-section * {
    color: #000000 !important;
}

/* Forzar todo a negro como √∫ltimo recurso */
.assistant-message [class*="st-emotion"],
.assistant-message [class*="e1rzn78k"],
.assistant-message [class*="erovr38"] {
    color: #000000 !important;
}

/* Asegurar que elementos espec√≠ficos sean negros */
div.stChatMessage.assistant-message * {
    color: #000000 !important;
}

/* Override para cualquier clase que Streamlit pueda agregar */
.assistant-message [class] {
    color: #000000 !important;
}

/* Modo oscuro deshabilitado para m√°ximo contraste */
@media (prefers-color-scheme: dark) {
    .assistant-message,
    .assistant-message * {
        background-color: #ffffff !important;
        color: #000000 !important;
    }
}
</style>
""", unsafe_allow_html=True)


class ChatbotFrontend:
    """Clase principal para el frontend del chatbot"""
    
    def __init__(self):
        self.backend_url = "http://localhost:8000"  # URL del backend FastAPI
        self.session_id = self._get_session_id()
        self.ollama_client = None
        self.interpreter = None
        
        # Inicializar estado de la sesi√≥n
        if 'messages' not in st.session_state:
            st.session_state.messages = []
        if 'prediction_history' not in st.session_state:
            st.session_state.prediction_history = []
        if 'current_context' not in st.session_state:
            st.session_state.current_context = {}
        if 'tool_results' not in st.session_state:
            st.session_state.tool_results = {
                'recent_predictions': [],
                'recent_comparisons': [],
                'recent_analysis': [],
                'last_action': None,
                'last_results': None
            }
    
    def _get_session_id(self) -> str:
        """Obtener o crear ID de sesi√≥n"""
        if 'session_id' not in st.session_state:
            st.session_state.session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        return st.session_state.session_id
    
    async def _init_ollama_client(self):
        """Inicializar cliente de Ollama con detecci√≥n autom√°tica de modelos"""
        if not OLLAMA_AVAILABLE:
            logger.warning("Ollama integration no disponible")
            return False
            
        if self.ollama_client is None:
            try:
                from chatbot.ollama_integration import OllamaClient, OllamaConfig
                
                # URL fija de Ollama
                ollama_url = "https://3200-34-168-28-225.ngrok-free.app"
                
                # Crear configuraci√≥n temporal para detectar modelos
                temp_config = OllamaConfig(
                    base_url=ollama_url,
                    model_name="llama3.2",  # Modelo por defecto
                    timeout=30,
                    max_tokens=1000,
                    temperature=0.7
                )
                
                # Crear cliente temporal
                temp_client = OllamaClient(temp_config)
                
                # Detectar modelos disponibles
                available_models = await self._detect_available_models(temp_client, ollama_url)
                
                if not available_models:
                    logger.warning("No se pudieron detectar modelos en Ollama")
                    return False
                
                # Seleccionar el mejor modelo
                best_model = self._select_best_model(available_models)
                logger.info(f"Usando modelo: {best_model} de {available_models}")
                
                # Crear configuraci√≥n final
                final_config = OllamaConfig(
                    base_url=ollama_url,
                    model_name=best_model,
                    timeout=120,
                    max_tokens=1000,
                    temperature=0.7
                )
                
                # Crear cliente final
                self.ollama_client = OllamaClient(final_config)
                
                # Verificar conexi√≥n final
                if await self.ollama_client.check_connection():
                    logger.info(f"Ollama conectado exitosamente con modelo {best_model}")
                    st.session_state.ollama_connected = True
                    st.session_state.ollama_model_used = best_model
                    return True
                else:
                    logger.warning("No se pudo conectar con Ollama")
                    self.ollama_client = None
                    return False
                    
            except ImportError as e:
                logger.error(f"Error importando Ollama (posiblemente falta aiohttp): {e}")
                return False
            except Exception as e:
                logger.error(f"Error inicializando Ollama: {e}")
                self.ollama_client = None
                return False
        
        return True

    async def _detect_available_models(self, client, base_url):
        """Detectar modelos disponibles en Ollama"""
        try:
            # Intentar importar aiohttp
            try:
                import aiohttp
            except ImportError:
                logger.error("aiohttp no est√° disponible. Inst√°lalo con: pip install aiohttp")
                return []
            
            async with aiohttp.ClientSession() as session:
                headers = {
                    "ngrok-skip-browser-warning": "true",
                    "Content-Type": "application/json"
                }
                
                async with session.get(
                    f"{base_url}/api/tags",
                    timeout=aiohttp.ClientTimeout(total=10),
                    headers=headers
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        models = [model.get('name', '').split(':')[0] for model in data.get('models', [])]
                        models = [m for m in models if m]  # Filtrar nombres vac√≠os
                        logger.info(f"Modelos detectados: {models}")
                        return models
                    else:
                        logger.warning(f"Error al obtener modelos: {response.status}")
                        return []
        except Exception as e:
            logger.error(f"Error detectando modelos: {e}")
            # Fallback con modelos comunes
            return ['llama3.2', 'llama3.1', 'llama3']

    def _select_best_model(self, available_models):
        """Seleccionar el mejor modelo basado en prioridad"""
        # Prioridad de modelos (del mejor al menos preferido)
        priority_models = [
            'llama3.2',
            'llama3.1', 
            'llama3',
            'llama2',
            'codellama',
            'mistral',
            'gemma',
            'phi',
            'qwen'
        ]
        
        # Buscar el primer modelo de la lista de prioridad que est√© disponible
        for preferred in priority_models:
            for available in available_models:
                if preferred.lower() in available.lower():
                    return available
        
        # Si no se encuentra ninguno preferido, usar el primero disponible
        if available_models:
            return available_models[0]
        
        # Fallback
        return "llama3.2"
    
    def render_sidebar(self):
        """Renderizar barra lateral con configuraciones"""
        st.sidebar.header("ü§ñ Configuraci√≥n del Chat")
        
        with st.sidebar.container():
            st.markdown('<div class="sidebar-section">', unsafe_allow_html=True)
            st.subheader("üîó Estado de Ollama")
            
            # Estado de conexi√≥n
            if st.session_state.get('ollama_connected', False):
                st.success(f"‚úÖ Conectado - Modelo: {st.session_state.get('ollama_model_used', 'N/A')}")
            else:
                st.warning("‚ö†Ô∏è No conectado")
            
            # URL de Ollama (fija)
            st.info("üåê URL: https://3200-34-168-28-225.ngrok-free.app")
            
            # Bot√≥n para reconectar
            if st.button("üîÑ Reconectar Ollama"):
                self.ollama_client = None
                st.session_state.ollama_connected = False
                try:
                    connected = asyncio.run(self._init_ollama_client())
                    if connected:
                        st.success("‚úÖ Reconectado exitosamente")
                        st.rerun()
                    else:
                        st.error("‚ùå Error al reconectar")
                except Exception as e:
                    st.error(f"‚ùå Error: {e}")
            
            st.markdown('</div>', unsafe_allow_html=True)
            
            # Configuraci√≥n de predicciones
            st.markdown('<div class="sidebar-section">', unsafe_allow_html=True)
            st.subheader("üìä Configuraci√≥n de Predicciones")
            
            # D√≠as para predicci√≥n
            prediction_days = st.slider(
                "D√≠as a predecir",
                min_value=7,
                max_value=90,
                value=st.session_state.get('prediction_days', 30),
                step=7,
                help="N√∫mero de d√≠as hacia el futuro para las predicciones"
            )
            st.session_state.prediction_days = prediction_days
            
            # Incluir intervalos de confianza
            include_confidence = st.checkbox(
                "Incluir intervalos de confianza",
                value=st.session_state.get('include_confidence', True),
                help="Mostrar rangos de confianza en las predicciones"
            )
            st.session_state.include_confidence = include_confidence
            
            # Usar cach√©
            use_cache = st.checkbox(
                "Usar cach√© de modelos",
                value=st.session_state.get('use_cache', True),
                help="Acelerar predicciones usando modelos en cach√©"
            )
            st.session_state.use_cache = use_cache
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # Historial de predicciones
        st.sidebar.header("ÔøΩ Historial")
        
        with st.sidebar.container():
            st.markdown('<div class="sidebar-section">', unsafe_allow_html=True)
            
            # Alcance de predicci√≥n
            scope_options = {
                "Producto √∫nico": PredictionScope.SINGLE_PRODUCT,
                "Categor√≠a": PredictionScope.CATEGORY,
                "Negocio": PredictionScope.BUSINESS,
                "Mercado": PredictionScope.MARKET
            }
            
            selected_scope = st.selectbox(
                "Alcance",
                options=list(scope_options.keys()),
                help="Alcance de la predicci√≥n"
            )
            st.session_state.prediction_scope = scope_options[selected_scope]
            
            # D√≠as de predicci√≥n
            prediction_days = st.slider(
                "D√≠as a predecir",
                min_value=1,
                max_value=365,
                value=30,
                help="N√∫mero de d√≠as hacia el futuro"
            )
            st.session_state.prediction_days = prediction_days
            
            # Tipo de modelo
            model_options = {
                "Auto-selecci√≥n": ModelType.AUTO_SELECT,
                "Lineal": ModelType.LINEAR,
                "Polinomial": ModelType.POLYNOMIAL
            }
            
            selected_model_type = st.selectbox(
                "Tipo de modelo",
                options=list(model_options.keys()),
                help="Modelo de ML a utilizar"
            )
            st.session_state.model_type = model_options[selected_model_type]
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # Historial de predicciones
        st.sidebar.header("üìà Historial")
        
        if st.session_state.prediction_history:
            for i, pred in enumerate(st.session_state.prediction_history[-5:]):
                with st.sidebar.expander(f"Predicci√≥n {i+1}"):
                    st.write(f"**Fecha:** {pred['timestamp']}")
                    st.write(f"**Confianza:** {pred['confidence']:.1%}")
                    st.write(f"**Modelo:** {pred['model']}")
        else:
            st.sidebar.info("No hay predicciones a√∫n")
        
        # Limpiar historial
        if st.sidebar.button("üóëÔ∏è Limpiar Chat"):
            st.session_state.messages = []
            st.session_state.prediction_history = []
            st.rerun()
    
    def render_chat_interface(self):
        """Renderizar interfaz principal de chat"""
        st.header("ü§ñ Asistente de Predicci√≥n de Demanda")
        st.markdown("Pregunta sobre predicciones, an√°lisis de tendencias y insights de tu negocio.")
        
        # Contenedor para mensajes
        chat_container = st.container()
        
        with chat_container:
            # Mostrar historial de mensajes
            for message in st.session_state.messages:
                self._render_message(message)
        
        # Input para nuevos mensajes
        user_input = st.chat_input("Escribe tu pregunta sobre predicci√≥n de demanda...")
        
        if user_input:
            # Agregar mensaje del usuario
            st.session_state.messages.append({
                "role": "user",
                "content": user_input,
                "timestamp": datetime.now()
            })
            
            # Procesar mensaje
            with st.spinner("Analizando y generando respuesta..."):
                response = self._process_user_message(user_input)
            
            # Agregar respuesta del asistente
            st.session_state.messages.append({
                "role": "assistant",
                "content": response,
                "timestamp": datetime.now()
            })
            
            st.rerun()
    
    def _render_message(self, message: Dict[str, Any]):
        """Renderizar un mensaje individual"""
        timestamp = message['timestamp'].strftime("%H:%M")
        
        if message['role'] == 'user':
            st.markdown(f"""
            <div class="stChatMessage user-message">
                <strong>T√∫ ({timestamp}):</strong> {message['content']}
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
            <div class="stChatMessage assistant-message">
                <strong>Asistente ({timestamp}):</strong> {message['content']}
            </div>
            """, unsafe_allow_html=True)
            
            # Si hay datos de predicci√≥n, mostrar visualizaci√≥n
            if 'prediction_data' in message:
                self._render_prediction_visualization(message['prediction_data'])
    
    def _render_prediction_visualization(self, prediction_data: Dict[str, Any]):
        """Renderizar visualizaci√≥n de predicci√≥n"""
        try:
            if not prediction_data or 'predicciones' not in prediction_data:
                return
            
            # Crear gr√°fico simple con Plotly
            import plotly.graph_objects as go
            
            predicciones = prediction_data.get('predicciones', [])
            if not predicciones:
                return
                
            dias = list(range(1, len(predicciones) + 1))
            
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=dias,
                y=predicciones,
                mode='lines+markers',
                name='Predicci√≥n',
                line=dict(color='#1976d2', width=3),
                marker=dict(size=6)
            ))
            
            fig.update_layout(
                title=f"Predicci√≥n para Producto {prediction_data.get('producto_id', 'N/A')}",
                xaxis_title="D√≠as",
                yaxis_title="Demanda",
                height=400,
                template="plotly_white"
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
        except Exception as e:
            logger.warning(f"Error renderizando visualizaci√≥n: {e}")

    def _process_user_message(self, user_input: str) -> str:
        """Procesar mensaje del usuario y generar respuesta"""
        
        # Intentar inicializar Ollama si no est√° disponible
        if not self.ollama_client and OLLAMA_AVAILABLE:
            try:
                # Usar asyncio.run para la inicializaci√≥n
                asyncio.run(self._init_ollama_client())
            except Exception as e:
                logger.warning(f"No se pudo inicializar Ollama: {e}")
        
        # PRIMERO: Detectar si es una pregunta de seguimiento
        if self._detect_follow_up_question(user_input):
            logger.info(f"Pregunta de seguimiento detectada: {user_input}")
            return self._get_contextual_response(user_input)
        
        # SEGUNDO: Detectar intenci√≥n del usuario para nuevas consultas
        intent = self._detect_intent(user_input)
        
        logger.info(f"Intent detectado: {intent} para input: {user_input}")
        
        # Manejar seg√∫n la intenci√≥n
        if intent == 'prediction':
            return self._handle_prediction_request(user_input)
        elif intent == 'comparison':
            return self._handle_model_comparison(user_input)
        elif intent == 'analysis':
            return self._handle_analysis_request(user_input)
        else:  # general
            return self._handle_general_chat(user_input)
    
    def _detect_intent(self, user_input: str) -> str:
        """Detectar la intenci√≥n del usuario de manera m√°s inteligente"""
        user_input_lower = user_input.lower()
        
        # Palabras clave m√°s espec√≠ficas para cada intenci√≥n
        prediction_keywords = [
            'predecir', 'predicci√≥n', 'pron√≥stico', 'demanda', 'ventas futuras', 'futuro',
            'pr√≥ximos', 'd√≠as', 'semanas', 'meses', 'cu√°nto', 'cu√°ntos',
            'producto', 'cu√°l ser√°', 'cuanto voy a vender', 'proyecci√≥n',
            'estimaci√≥n', 'forecast', 'prever', 'proyectar'
        ]
        
        # Palabras espec√≠ficas para comparaci√≥n (m√°s restrictivas)
        comparison_keywords = [
            'comparar modelos', 'mejor modelo', 'qu√© modelo', 'cu√°l modelo',
            'modelo m√°s preciso', 'modelo m√°s exacto', 'accuracy', 'precisi√≥n del modelo',
            'performance modelo', 'rendimiento modelo', 'evaluar modelos',
            'algoritmo mejor', 'ml accuracy'
        ]
        
        # Palabras para an√°lisis de tendencias
        analysis_keywords = [
            'analizar tendencia', 'tendencia de ventas', 'patr√≥n', 'insight',
            'estudiar', 'examinar tendencia', 'revisar tendencia', 'investigar patr√≥n',
            'reportar tendencia', 'reporte', 'an√°lisis hist√≥rico', 'comportamiento',
            'categor√≠a', 'an√°lisis de categor√≠a'
        ]
        
        # Conversaci√≥n general
        general_keywords = [
            'hola', 'buenos', 'buenas', 'hey', 'hi', 'hello', 'qu√© haces',
            'que haces', 'gracias', 'thanks', 'ayuda', 'help', 'qu√© puedes',
            'capacidades', 'como funciona', 'c√≥mo funciona'
        ]
        
        # Detecci√≥n espec√≠fica por frases exactas (mayor prioridad)
        if any(phrase in user_input_lower for phrase in ['comparar modelos', 'qu√© modelo', 'cu√°l modelo', 'mejor modelo']):
            return 'comparison'
        
        if any(phrase in user_input_lower for phrase in ['analizar tendencia', 'tendencia de', 'an√°lisis de']):
            return 'analysis'
        
        # Buscar con n√∫meros de producto (indica predicci√≥n)
        import re
        if re.search(r'producto\s+\d+', user_input_lower) or re.search(r'\d+\s+d√≠as?', user_input_lower):
            return 'prediction'
        
        if re.search(r'demanda.*producto', user_input_lower) or re.search(r'cu√°l ser√°.*demanda', user_input_lower):
            return 'prediction'
        
        # Contar coincidencias por categor√≠a
        prediction_count = sum(1 for keyword in prediction_keywords if keyword in user_input_lower)
        comparison_count = sum(1 for keyword in comparison_keywords if keyword in user_input_lower)
        analysis_count = sum(1 for keyword in analysis_keywords if keyword in user_input_lower)
        general_count = sum(1 for keyword in general_keywords if keyword in user_input_lower)
        
        # Decidir basado en la mayor cantidad de coincidencias
        counts = {
            'prediction': prediction_count,
            'comparison': comparison_count,
            'analysis': analysis_count,
            'general': general_count
        }
        
        max_count = max(counts.values())
        if max_count == 0:
            return 'general'  # Si no hay coincidencias claras, asumir conversaci√≥n general
        
        # Retornar la intenci√≥n con m√°s coincidencias
        for intent, count in counts.items():
            if count == max_count:
                return intent
        
        return 'general'
    
    def _save_tool_result(self, tool_type: str, result_data: Dict[str, Any], user_input: str):
        """Guardar resultados de herramientas para contexto futuro"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        if tool_type == 'prediction':
            prediction_summary = {
                'timestamp': timestamp,
                'user_query': user_input,
                'producto_id': result_data.get('producto_id', 'N/A'),
                'prediccion_promedio': result_data.get('predicciones', [0])[-1] if result_data.get('predicciones') else 0,
                'modelo_usado': result_data.get('mejor_modelo', 'N/A'),
                'confianza': result_data.get('confianza', 0),
                'tendencia': result_data.get('trend_type', 'N/A'),
                'raw_data': result_data
            }
            st.session_state.tool_results['recent_predictions'].append(prediction_summary)
            # Mantener solo las √∫ltimas 3
            if len(st.session_state.tool_results['recent_predictions']) > 3:
                st.session_state.tool_results['recent_predictions'].pop(0)
        
        elif tool_type == 'comparison':
            comparison_summary = {
                'timestamp': timestamp,
                'user_query': user_input,
                'mejor_modelo': result_data.get('best_model', 'Random Forest'),
                'r2_scores': result_data.get('r2_scores', {}),
                'conclusion': result_data.get('conclusion', 'Comparaci√≥n realizada'),
                'raw_data': result_data
            }
            st.session_state.tool_results['recent_comparisons'].append(comparison_summary)
            # Mantener solo las √∫ltimas 2
            if len(st.session_state.tool_results['recent_comparisons']) > 2:
                st.session_state.tool_results['recent_comparisons'].pop(0)
        
        elif tool_type == 'analysis':
            analysis_summary = {
                'timestamp': timestamp,
                'user_query': user_input,
                'tipo_analisis': result_data.get('analysis_type', 'general'),
                'categoria': result_data.get('category', 'N/A'),
                'insights': result_data.get('insights', []),
                'raw_data': result_data
            }
            st.session_state.tool_results['recent_analysis'].append(analysis_summary)
            # Mantener solo las √∫ltimas 2
            if len(st.session_state.tool_results['recent_analysis']) > 2:
                st.session_state.tool_results['recent_analysis'].pop(0)
        
        # Actualizar √∫ltimo resultado
        st.session_state.tool_results['last_action'] = tool_type
        st.session_state.tool_results['last_results'] = result_data

    def _detect_follow_up_question(self, user_input: str) -> bool:
        """Detectar si es una pregunta de seguimiento sobre resultados anteriores"""
        follow_up_patterns = [
            'qu√© puedes decir al respecto',
            'qu√© opinas sobre esto',
            'qu√© significa esto',
            'expl√≠came esto',
            'analiza esto',
            'interpreta esto',
            'qu√© conclusiones',
            'qu√© recomendaciones',
            'bas√°ndote en esto',
            'sobre estos resultados',
            'sobre esta informaci√≥n',
            'que me dices de',
            'c√≥mo interpretas',
            'qu√© piensas'
        ]
        
        user_lower = user_input.lower()
        return any(pattern in user_lower for pattern in follow_up_patterns)

    def _get_contextual_response(self, user_input: str) -> str:
        """Generar respuesta contextual basada en resultados anteriores"""
        last_action = st.session_state.tool_results.get('last_action')
        last_results = st.session_state.tool_results.get('last_results')
        
        if not last_action or not last_results:
            return self._get_intelligent_fallback(user_input)
        
        user_lower = user_input.lower()
        
        if last_action == 'comparison':
            return self._analyze_model_comparison_context(user_input, last_results)
        elif last_action == 'prediction':
            return self._analyze_prediction_context(user_input, last_results)
        elif last_action == 'analysis':
            return self._analyze_analysis_context(user_input, last_results)
        
        return self._get_intelligent_fallback(user_input)

    def _analyze_model_comparison_context(self, user_input: str, comparison_data: Dict) -> str:
        """Analizar y explicar resultados de comparaci√≥n de modelos"""
        try:
            # Obtener el √∫ltimo resultado de comparaci√≥n
            recent_comp = st.session_state.tool_results['recent_comparisons'][-1] if st.session_state.tool_results['recent_comparisons'] else None
            
            if not recent_comp:
                return "No encontr√© resultados de comparaci√≥n recientes para analizar."
            
            response = f"""
            <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #7b1fa2; color: #000000;">
            
            ## ü§î An√°lisis de la Comparaci√≥n de Modelos
            
            **Bas√°ndome en tu pregunta:** "{user_input}"
            
            ### üéØ Interpretaci√≥n de Resultados
            
            <div style="background-color: #e8f5e8; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #4caf50; color: #1b5e20; font-weight: 600;">
            üèÜ **Modelo Ganador:** {recent_comp['mejor_modelo']}
            </div>
            
            ### üìä ¬øQu√© significan estos n√∫meros?
            
            **R¬≤ Score (Coeficiente de Determinaci√≥n):**
            - üìà Mide qu√© tan bien el modelo explica la variabilidad de tus datos
            - üéØ Rango: 0.0 (terrible) a 1.0 (perfecto)
            - ‚úÖ **Mayor R¬≤ = Mejor predicci√≥n**
            
            **MSE (Error Cuadr√°tico Medio):**
            - üìâ Promedio de errores al cuadrado
            - üéØ **Menor MSE = Mejor precisi√≥n**
            
            **MAE (Error Absoluto Medio):**
            - üìä Error promedio sin elevar al cuadrado
            - üéØ **Menor MAE = Predicciones m√°s cercanas**
            
            ### üí° Recomendaciones Pr√°cticas
            
            <div style="background-color: #f3e5f5; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #9c27b0; color: #4a148c; font-weight: 600;">
            üöÄ **Para tu negocio:** El {recent_comp['mejor_modelo']} te dar√° las predicciones m√°s confiables
            </div>
            
            **Pr√≥ximos pasos sugeridos:**
            1. üìä Usa el {recent_comp['mejor_modelo']} para predicciones futuras
            2. üîÑ Re-eval√∫a mensualmente con nuevos datos
            3. üìà Monitora la precisi√≥n en la pr√°ctica
            
            ### ü§ñ ¬øQuieres que realice alg√∫n an√°lisis espec√≠fico?
            
            Puedo ayudarte con:
            - üìä Predicci√≥n de demanda usando el mejor modelo
            - üìà An√°lisis de tendencias espec√≠ficas
            - üéØ Recomendaciones personalizadas para tu inventario
            
            </div>
            """
            
            return response
            
        except Exception as e:
            return f"Error analizando la comparaci√≥n: {str(e)}"

    def _analyze_prediction_context(self, user_input: str, prediction_data: Dict) -> str:
        """Analizar y explicar resultados de predicci√≥n"""
        try:
            recent_pred = st.session_state.tool_results['recent_predictions'][-1] if st.session_state.tool_results['recent_predictions'] else None
            
            if not recent_pred:
                return "No encontr√© predicciones recientes para analizar."
            
            response = f"""
            <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #1976d2; color: #000000;">
            
            ## üìä An√°lisis de tu Predicci√≥n de Demanda
            
            **Respondiendo a:** "{user_input}"
            
            ### üéØ Resumen de Resultados
            
            <div style="background-color: #e3f2fd; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #1976d2; color: #0d47a1; font-weight: 600;">
            üì¶ **Producto {recent_pred['producto_id']}:** {recent_pred['prediccion_promedio']:.1f} unidades promedio
            </div>
            
            ### üîç Interpretaci√≥n Detallada
            
            **Confianza del Modelo:**
            - üéØ **{recent_pred['confianza']:.1%}** de confianza en la predicci√≥n
            - ü§ñ Modelo usado: **{recent_pred['modelo_usado']}**
            - üìà Tendencia detectada: **{recent_pred['tendencia']}**
            
            ### üí° ¬øQu√© significa para tu negocio?
            
            **Planificaci√≥n de Inventario:**
            """
            
            # Agregar recomendaciones basadas en la predicci√≥n
            prediccion = recent_pred['prediccion_promedio']
            if prediccion > 100:
                response += """
            <div style="background-color: #fff3e0; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #ff9800; color: #e65100; font-weight: 600;">
            üî• **Alta demanda proyectada** - Considera aumentar tu inventario
            </div>
            """
            elif prediccion > 50:
                response += """
            <div style="background-color: #e8f5e8; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #4caf50; color: #1b5e20; font-weight: 600;">
            ‚úÖ **Demanda moderada** - Mant√©n niveles de stock normales
            </div>
            """
            else:
                response += """
            <div style="background-color: #fce4ec; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #e91e63; color: #880e4f; font-weight: 600;">
            üìâ **Demanda baja** - Eval√∫a promociones o reducir inventario
            </div>
            """
            
            response += f"""
            ### üìã Acciones Recomendadas
            
            1. üìä **Stock √≥ptimo:** {prediccion * 1.2:.0f} unidades (20% buffer)
            2. üîÑ **Punto de reorden:** {prediccion * 0.3:.0f} unidades
            3. üìÖ **Pr√≥xima revisi√≥n:** En 1 semana
            
            ### ü§ñ ¬øTe ayudo con algo m√°s?
            
            Puedo ayudarte a:
            - üîç Comparar con otros productos
            - üìà Analizar tendencias hist√≥ricas
            - üí∞ Calcular rentabilidad proyectada
            
            </div>
            """
            
            return response
            
        except Exception as e:
            return f"Error analizando la predicci√≥n: {str(e)}"

    def _analyze_analysis_context(self, user_input: str, analysis_data: Dict) -> str:
        """Analizar y explicar resultados de an√°lisis general"""
        try:
            recent_analysis = st.session_state.tool_results['recent_analysis'][-1] if st.session_state.tool_results['recent_analysis'] else None
            
            if not recent_analysis:
                return "No encontr√© an√°lisis recientes para interpretar."
            
            response = f"""
            <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #ff9800; color: #000000;">
            
            ## üìà Interpretaci√≥n del An√°lisis
            
            **Tu consulta:** "{user_input}"
            
            ### üéØ Resumen del An√°lisis Realizado
            
            <div style="background-color: #fff3e0; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #ff9800; color: #e65100; font-weight: 600;">
            üìä **Tipo:** {recent_analysis['tipo_analisis']} | **Categor√≠a:** {recent_analysis['categoria']}
            </div>
            
            ### üí° Insights Clave
            """
            
            for insight in recent_analysis.get('insights', [])[:3]:  # Mostrar m√°ximo 3
                response += f"""
            - üéØ {insight}
            """
            
            response += """
            
            ### üöÄ Pr√≥ximos Pasos Sugeridos
            
            1. üìä Monitorear tendencias identificadas
            2. üîÑ Ajustar estrategias seg√∫n insights
            3. üìà Revisar resultados en 2 semanas
            
            ### ü§ñ ¬øNecesitas m√°s detalles?
            
            Puedo profundizar en:
            - üìä An√°lisis espec√≠ficos por producto
            - üîç Comparaciones detalladas
            - üí° Recomendaciones personalizadas
            
            </div>
            """
            
            return response
            
        except Exception as e:
            return f"Error analizando el an√°lisis: {str(e)}"

    def _handle_prediction_request(self, user_input: str) -> str:
        """Manejar solicitud de predicci√≥n"""
        try:
            # Extraer par√°metros de la sesi√≥n
            product_id = self._extract_product_id(user_input)
            
            # Crear petici√≥n (formato compatible con backend)
            request_data = {
                "producto_id": product_id,
                "dias_adelante": st.session_state.get('prediction_days', 30),
                "include_confidence": True,
                "use_cache": True
            }
            
            # Intentar llamar al backend
            try:
                response = requests.post(
                    f"{self.backend_url}/api/predict/demanda",
                    json=request_data,
                    timeout=5  # Timeout corto para demo
                )
                
                if response.status_code == 200:
                    prediction_data = response.json()
                    
                    # Guardar en historial
                    st.session_state.prediction_history.append({
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
                        "confidence": prediction_data.get('confianza', 0),
                        "model": prediction_data.get('mejor_modelo', 'unknown'),
                        "data": prediction_data
                    })
                    
                    # Guardar resultado para contexto futuro
                    self._save_tool_result('prediction', prediction_data, user_input)
                    
                    # Generar respuesta interpretativa
                    return self._generate_prediction_interpretation(prediction_data, user_input)
                else:
                    # Backend responde pero hay error, usar datos simulados
                    return self._generate_demo_prediction(product_id, user_input)
                    
            except (requests.RequestException, requests.Timeout):
                # Backend no disponible, usar datos simulados
                return self._generate_demo_prediction(product_id, user_input)
                
        except Exception as e:
            return f"Error procesando predicci√≥n: {str(e)}"
    
    def _generate_demo_prediction(self, product_id: int, user_input: str) -> str:
        """Generar predicci√≥n de demostraci√≥n cuando el backend no est√° disponible"""
        import numpy as np
        import hashlib
        
        # Usar hash del input para generar datos √∫nicos pero consistentes
        seed_value = int(hashlib.md5(f"{product_id}_{user_input}".encode()).hexdigest()[:8], 16) % 10000
        np.random.seed(seed_value)
        
        days = st.session_state.get('prediction_days', 30)
        
        # Crear predicci√≥n simulada m√°s variada
        base_demand = 30 + (product_id % 20) * 5 + np.random.randint(-10, 20)
        
        # Diferentes tipos de tendencia basados en el producto
        trend_types = ['creciente', 'decreciente', 'estacional', 'estable']
        trend_type = trend_types[product_id % 4]
        
        predicted_values = []
        for i in range(days):
            if trend_type == 'creciente':
                trend_value = base_demand + (i * 0.5) + np.random.normal(0, 2)
            elif trend_type == 'decreciente':
                trend_value = base_demand - (i * 0.3) + np.random.normal(0, 2)
            elif trend_type == 'estacional':
                # Patr√≥n semanal
                seasonal = 15 * np.sin(2 * np.pi * i / 7) + 5 * np.cos(2 * np.pi * i / 14)
                trend_value = base_demand + seasonal + np.random.normal(0, 3)
            else:  # estable
                trend_value = base_demand + np.random.normal(0, 4)
            
            value = max(1, int(trend_value))
            predicted_values.append(value)
        
        # Calcular m√©tricas m√°s realistas
        avg_demand = np.mean(predicted_values)
        trend = trend_type
        
        # Confidence basada en tipo de tendencia
        confidence_base = {
            'estable': 0.85,
            'creciente': 0.78,
            'decreciente': 0.72,
            'estacional': 0.68
        }
        
        confidence = confidence_base[trend_type] + np.random.uniform(-0.08, 0.08)
        confidence = max(0.6, min(0.95, confidence))
        
        # Seleccionar modelo basado en tendencia
        model_selection = {
            'estable': 'linear',
            'creciente': 'linear', 
            'decreciente': 'polynomial',
            'estacional': 'random_forest'
        }
        
        selected_model = model_selection[trend_type]
        
        prediction_data = {
            'predicciones': predicted_values,
            'confianza': confidence,
            'mejor_modelo': selected_model,
            'dias_adelante': days,
            'producto_id': product_id,
            'trend_type': trend_type
        }
        
        # Guardar resultados en el contexto de herramientas
        self._save_tool_result('prediction', prediction_data, user_input)
        
        # Guardar en historial
        st.session_state.prediction_history.append({
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
            "confidence": confidence,
            "model": selected_model,
            "trend": trend_type,
            "data": prediction_data
        })
        
        # Agregar nota de demostraci√≥n
        demo_note = "üìç **Modo Demo**: Datos simulados (backend en desarrollo)"
        interpretation = self._generate_prediction_interpretation(prediction_data, user_input)
        
        return f"{demo_note}\n\n{interpretation}"

    def _handle_model_comparison(self, user_input: str) -> str:
        """Manejar comparaci√≥n de modelos con fallback inteligente"""
        try:
            # Intentar obtener comparaci√≥n del backend
            comparison_id = f"comp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            response = requests.get(
                f"{self.backend_url}/api/predict/models/comparison/{comparison_id}",
                timeout=5  # Timeout corto
            )
            
            if response.status_code == 200:
                comparison_data = response.json()
                
                # Guardar resultado para contexto futuro
                self._save_tool_result('comparison', comparison_data, user_input)
                
                return self._generate_comparison_interpretation(comparison_data, user_input)
            else:
                # Si falla el backend, generar comparaci√≥n simulada
                return self._generate_demo_comparison(user_input)
                
        except Exception as e:
            logger.warning(f"Error en comparaci√≥n del backend: {e}")
            return self._generate_demo_comparison(user_input)

    def _generate_demo_comparison(self, user_input: str) -> str:
        """Generar comparaci√≥n de modelos simulada"""
        import numpy as np
        
        # Datos simulados de comparaci√≥n
        models_comparison = {
            'linear': {
                'mse': np.random.uniform(0.15, 0.25),
                'r2': np.random.uniform(0.75, 0.85),
                'mae': np.random.uniform(0.12, 0.18),
                'speed': 'Muy r√°pido',
                'complexity': 'Baja'
            },
            'polynomial': {
                'mse': np.random.uniform(0.10, 0.20),
                'r2': np.random.uniform(0.80, 0.90),
                'mae': np.random.uniform(0.08, 0.15),
                'speed': 'Moderado',
                'complexity': 'Media'
            },
            'random_forest': {
                'mse': np.random.uniform(0.08, 0.15),
                'r2': np.random.uniform(0.85, 0.95),
                'mae': np.random.uniform(0.06, 0.12),
                'speed': 'Lento',
                'complexity': 'Alta'
            }
        }
        
        # Encontrar el mejor modelo por R¬≤
        best_model = max(models_comparison.keys(), key=lambda k: models_comparison[k]['r2'])
        
        # Guardar resultados en el contexto
        comparison_result = {
            'best_model': best_model.replace('_', ' ').title(),
            'r2_scores': {k: v['r2'] for k, v in models_comparison.items()},
            'models_data': models_comparison,
            'conclusion': f"{best_model.replace('_', ' ').title()} es el m√°s preciso"
        }
        self._save_tool_result('comparison', comparison_result, user_input)
        
        interpretation = f"""
        <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #e0e0e0; color: #000000;">
        
        ## üîç Comparaci√≥n de Modelos de ML
        
        **Bas√°ndome en tu consulta:** "{user_input}"
        
        ### üèÜ Modelo Recomendado: **{best_model.replace('_', ' ').title()}**
        
        ### üìä Resultados Detallados
        
        """
        
        for model, metrics in models_comparison.items():
            model_name = model.replace('_', ' ').title()
            r2_color = "#1b5e20" if metrics['r2'] > 0.85 else "#f57c00" if metrics['r2'] > 0.75 else "#d32f2f"
            
            interpretation += f"""
        **ü§ñ {model_name}:**
        - **R¬≤ Score**: <span style="color: {r2_color}; font-weight: bold; background-color: #f5f5f5; padding: 2px 6px; border-radius: 4px;">{metrics['r2']:.3f}</span>
        - **MSE**: <span style="color: #000000; font-weight: bold;">{metrics['mse']:.3f}</span>
        - **MAE**: <span style="color: #000000; font-weight: bold;">{metrics['mae']:.3f}</span>
        - **Velocidad**: <span style="color: #4a148c; font-weight: bold;">{metrics['speed']}</span>
        - **Complejidad**: <span style="color: #4a148c; font-weight: bold;">{metrics['complexity']}</span>
        
        """
        
        interpretation += f"""
        ### üí° Recomendaciones
        
        <div style="background-color: #e8f5e8; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #4caf50; color: #1b5e20; font-weight: 600;">
        üéØ **Mejor opci√≥n:** {best_model.replace('_', ' ').title()} con R¬≤ de {models_comparison[best_model]['r2']:.3f}
        </div>
        
        <div style="background-color: #f3e5f5; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #9c27b0; color: #4a148c; font-weight: 600;">
        üìà **Criterio de selecci√≥n:** Mayor R¬≤ indica mejor capacidad predictiva
        </div>
        
        ### üìã Gu√≠a de Selecci√≥n:
        - **Linear**: Ideal para tendencias simples y predicciones r√°pidas
        - **Polynomial**: Mejor para patrones m√°s complejos
        - **Random Forest**: M√°xima precisi√≥n para datos complejos
        
        </div>
        """
        
        return interpretation
    
    def _handle_analysis_request(self, user_input: str) -> str:
        """Manejar solicitud de an√°lisis con capacidades mejoradas"""
        
        # Extraer informaci√≥n de la consulta
        category = self._extract_category_from_input(user_input)
        
        if category:
            return self._generate_category_analysis(category, user_input)
        else:
            return self._generate_general_analysis(user_input)

    def _extract_category_from_input(self, user_input: str) -> str:
        """Extraer categor√≠a del input del usuario"""
        user_input_lower = user_input.lower()
        
        categories = {
            'electr√≥nicos': ['electronico', 'electronica', 'electronic', 'tecnologia'],
            'ropa': ['ropa', 'vestimenta', 'clothing', 'textil'],
            'alimentaci√≥n': ['alimento', 'comida', 'food', 'bebida'],
            'hogar': ['hogar', 'casa', 'home', 'domestico'],
            'deportes': ['deporte', 'sport', 'fitness', 'ejercicio'],
            'salud': ['salud', 'health', 'medicina', 'farmacia'],
            'belleza': ['belleza', 'beauty', 'cosmetico', 'maquillaje']
        }
        
        for category, keywords in categories.items():
            if any(keyword in user_input_lower for keyword in keywords):
                return category
        
        return None

    def _generate_category_analysis(self, category: str, user_input: str) -> str:
        """Generar an√°lisis de tendencias para una categor√≠a espec√≠fica"""
        import numpy as np
        
        # Generar datos simulados para la categor√≠a
        np.random.seed(hash(category) % 1000)
        
        # Simular tendencias de los √∫ltimos 6 meses
        months = ['Enero', 'Febrero', 'Marzo', 'Abril', 'Mayo', 'Junio']
        base_sales = np.random.randint(1000, 5000)
        
        # Generar tendencia (creciente, decreciente o estable)
        trend_type = np.random.choice(['creciente', 'decreciente', 'estable'])
        
        sales_data = []
        for i in range(6):
            if trend_type == 'creciente':
                variation = 1 + (i * 0.1) + np.random.uniform(-0.05, 0.05)
            elif trend_type == 'decreciente':
                variation = 1 - (i * 0.08) + np.random.uniform(-0.05, 0.05)
            else:  # estable
                variation = 1 + np.random.uniform(-0.1, 0.1)
            
            sales_data.append(int(base_sales * variation))
        
        # Calcular estad√≠sticas
        avg_sales = np.mean(sales_data)
        growth_rate = ((sales_data[-1] - sales_data[0]) / sales_data[0]) * 100
        
        trend_color = "#1b5e20" if growth_rate > 5 else "#d32f2f" if growth_rate < -5 else "#f57c00"
        trend_bg = "#e8f5e8" if growth_rate > 5 else "#ffebee" if growth_rate < -5 else "#fff3e0"
        
        interpretation = f"""
        <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #e0e0e0; color: #000000;">
        
        ## üìà An√°lisis de Tendencias: {category.title()}
        
        **Consulta analizada:** "{user_input}"
        
        ### üìä Resumen Ejecutivo
        
        <div style="background-color: {trend_bg}; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid {trend_color.replace('#', '').replace('1b5e20', '#4caf50').replace('d32f2f', '#f44336').replace('f57c00', '#ff9800')};">
        <strong style="color: {trend_color};">Tendencia General: {trend_type.title()}</strong><br>
        <strong style="color: #000000;">Crecimiento: {growth_rate:+.1f}%</strong><br>
        <strong style="color: #000000;">Ventas Promedio: {avg_sales:,.0f} unidades/mes</strong>
        </div>
        
        ### üìÖ Datos Hist√≥ricos (√öltimos 6 Meses)
        
        """
        
        for i, (month, sales) in enumerate(zip(months, sales_data)):
            change = ""
            if i > 0:
                change_pct = ((sales - sales_data[i-1]) / sales_data[i-1]) * 100
                change_color = "#1b5e20" if change_pct > 0 else "#d32f2f"
                change = f" <span style='color: {change_color}; font-weight: bold;'>({change_pct:+.1f}%)</span>"
            
            interpretation += f"- **{month}**: <span style='color: #000000; font-weight: bold;'>{sales:,} unidades</span>{change}\n"
        
        interpretation += f"""
        
        ### üîç Insights Clave
        
        """
        
        if trend_type == 'creciente':
            interpretation += f"""
        - üìà **Crecimiento sostenido**: La categor√≠a {category} muestra una tendencia positiva
        - üéØ **Oportunidad**: Considera aumentar inventario gradualmente
        - üí° **Estrategia**: Aprovecha el momentum con campa√±as de marketing
        """
        elif trend_type == 'decreciente':
            interpretation += f"""
        - üìâ **Declive observado**: La categor√≠a {category} est√° perdiendo tracci√≥n
        - ‚ö†Ô∏è **Alerta**: Revisa estrategias de precio y promoci√≥n
        - üîÑ **Acci√≥n**: Considera diversificar o renovar productos
        """
        else:
            interpretation += f"""
        - üìä **Estabilidad**: La categor√≠a {category} mantiene ventas consistentes
        - üéØ **Oportunidad**: Mercado maduro ideal para optimizaci√≥n
        - üí° **Estrategia**: Enf√≥cate en eficiencia y m√°rgenes
        """
        
        interpretation += f"""
        
        ### üí° Recomendaciones Espec√≠ficas
        
        <div style="background-color: #f3e5f5; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #9c27b0; color: #4a148c; font-weight: 600;">
        üéØ **Pr√≥ximos pasos:** Usa predicciones espec√≠ficas por producto para planificar inventario
        </div>
        
        <div style="background-color: #e3f2fd; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #2196f3; color: #0d47a1; font-weight: 600;">
        üìä **Tip:** Pregunta "¬øCu√°l ser√° la demanda del producto X en los pr√≥ximos 30 d√≠as?" para an√°lisis espec√≠fico
        </div>
        
        </div>
        """
        
        return interpretation

    def _handle_general_chat(self, user_input: str) -> str:
        """Manejar chat general con Ollama"""
        try:
            # Si hay cliente Ollama disponible, usar IA para responder
            if self.ollama_client:
                # Crear contexto sobre las herramientas disponibles
                system_context = """
                Eres un asistente especializado en predicci√≥n de demanda y an√°lisis de micronegocios.
                
                HERRAMIENTAS DISPONIBLES:
                1. Predicci√≥n de demanda: Puedo predecir la demanda futura de productos espec√≠ficos
                2. Comparaci√≥n de modelos: Puedo comparar diferentes modelos de ML para encontrar el m√°s preciso
                3. An√°lisis de tendencias: Puedo analizar patrones en los datos hist√≥ricos
                4. Reportes de inventario: Puedo generar reportes sobre estado del inventario
                
                IMPORTANTE: 
                - Si el usuario pregunta sobre predicciones, demanda, ventas futuras o productos espec√≠ficos, sugiere usar las herramientas de predicci√≥n
                - Si pregunta sobre modelos o precisi√≥n, sugiere comparar modelos
                - Para conversaci√≥n general, responde de manera amigable y √∫til
                - Siempre ofrece ayuda espec√≠fica relacionada con el negocio
                
                Responde de manera conversacional y √∫til. Si detectas que necesitan usar alguna herramienta espec√≠fica, gu√≠alos hacia esa funcionalidad.
                """
                
                # Usar asyncio para la llamada a Ollama
                try:
                    response = asyncio.run(self._get_ollama_response(user_input, system_context))
                    return response
                except Exception as e:
                    logger.warning(f"Error con Ollama: {e}")
                    return self._get_intelligent_fallback(user_input)
            else:
                return self._get_intelligent_fallback(user_input)
                
        except Exception as e:
            return f"Disculpa, hubo un error procesando tu mensaje. ¬øPodr√≠as reformular tu pregunta?"

    async def _get_ollama_response(self, user_input: str, system_context: str) -> str:
        """Obtener respuesta de Ollama de manera as√≠ncrona"""
        try:
            if not self.ollama_client:
                return self._get_intelligent_fallback(user_input)
            
            # Preparar el contexto de conversaci√≥n
            conversation_context = self._build_conversation_context()
            
            # Crear el prompt completo
            full_prompt = f"""
            {system_context}
            
            CONTEXTO DE CONVERSACI√ìN:
            {conversation_context}
            
            USUARIO: {user_input}
            
            ASISTENTE: """
            
            # Llamar a Ollama con los par√°metros correctos
            response = await self.ollama_client.generate_response(
                prompt=full_prompt,
                session_id=self.session_id,
                include_ml_context=True,
                stream=False
            )
            
            return response.strip()
            
        except Exception as e:
            logger.error(f"Error en respuesta de Ollama: {e}")
            return self._get_intelligent_fallback(user_input)

    def _build_conversation_context(self) -> str:
        """Construir contexto de conversaci√≥n enriquecido para Mistral"""
        if not st.session_state.messages:
            return "Esta es una nueva conversaci√≥n."
        
        # Tomar los √∫ltimos 5 mensajes para contexto m√°s rico
        recent_messages = st.session_state.messages[-5:] if len(st.session_state.messages) > 5 else st.session_state.messages
        
        context_parts = []
        
        # Construir contexto basado en mensajes
        for msg in recent_messages:
            role = "Usuario" if msg['role'] == 'user' else "Asistente"
            content = msg['content']
            
            # Limitar longitud del contenido para contexto
            content_summary = content[:200] + "..." if len(content) > 200 else content
            context_parts.append(f"{role}: {content_summary}")
        
        context = "\n".join(context_parts)
        
        # Agregar informaci√≥n sobre resultados recientes de herramientas
        tool_results = st.session_state.tool_results
        
        if (tool_results['recent_predictions'] or 
            tool_results['recent_comparisons'] or 
            tool_results['recent_analysis']):
            
            context += "\n\n=== RESULTADOS RECIENTES DE HERRAMIENTAS ==="
            
            # Predicciones recientes
            if tool_results['recent_predictions']:
                context += "\n\nPREDICCIONES REALIZADAS:"
                for pred in tool_results['recent_predictions'][-2:]:  # √öltimas 2
                    context += f"\n- Producto {pred['producto_id']}: {pred['prediccion_promedio']:.1f} unidades, modelo {pred['modelo_usado']}, confianza {pred['confianza']:.1%}, tendencia {pred['tendencia']}"
            
            # Comparaciones recientes
            if tool_results['recent_comparisons']:
                context += "\n\nCOMPARACIONES DE MODELOS:"
                for comp in tool_results['recent_comparisons'][-1:]:  # √öltima comparaci√≥n
                    context += f"\n- Modelo ganador: {comp['mejor_modelo']}"
                    context += f"\n- Conclusi√≥n: {comp['conclusion']}"
            
            # An√°lisis recientes
            if tool_results['recent_analysis']:
                context += "\n\nAN√ÅLISIS REALIZADOS:"
                for analysis in tool_results['recent_analysis'][-1:]:
                    context += f"\n- Tipo: {analysis['tipo_analisis']} para {analysis['categoria']}"
            
            # Informaci√≥n sobre la √∫ltima acci√≥n
            if tool_results['last_action']:
                context += f"\n\n√öLTIMA ACCI√ìN: {tool_results['last_action']}"
        
        return context

    def _get_intelligent_fallback(self, user_input: str) -> str:
        """Respuesta inteligente de fallback basada en an√°lisis del input"""
        user_input_lower = user_input.lower()
        
        # Saludos y conversaci√≥n general
        if any(word in user_input_lower for word in ['hola', 'buenos', 'buenas', 'hey', 'hi']):
            return """¬°Hola! üëã Soy tu asistente de an√°lisis de demanda para micronegocios. 

Puedo ayudarte con:
- üìä **Predicciones de demanda** para productos espec√≠ficos
- üîç **Comparaci√≥n de modelos** para encontrar el m√°s preciso
- üìà **An√°lisis de tendencias** en tus datos de ventas
- üìã **Reportes de inventario** y recomendaciones

¬øEn qu√© puedo ayudarte hoy?"""

        # Preguntas sobre capacidades
        elif any(word in user_input_lower for word in ['qu√© puedes', 'que haces', 'ayuda', 'help', 'capacidades']):
            return """ü§ñ **Mis Capacidades:**

**üìä Predicci√≥n de Demanda:**
- Predigo ventas futuras de productos espec√≠ficos
- Uso m√∫ltiples modelos de ML para mayor precisi√≥n
- Proporciono intervalos de confianza

**üîç An√°lisis Avanzado:**
- Comparo diferentes modelos para encontrar el m√°s preciso
- Identifico tendencias y patrones estacionales
- Genero insights accionables para tu negocio

**üí° Ejemplos de lo que puedes preguntarme:**
- "¬øCu√°l ser√° la demanda del producto 1 en los pr√≥ximos 30 d√≠as?"
- "¬øQu√© modelo es m√°s preciso para mis productos?"
- "Analiza las tendencias de ventas del √∫ltimo mes"

¬øQu√© te gustar√≠a explorar?"""

        # Agradecimientos
        elif any(word in user_input_lower for word in ['gracias', 'thanks', 'thank you']):
            return """¬°De nada! üòä 

Estoy aqu√≠ para ayudarte con el an√°lisis de tu negocio. ¬øHay algo m√°s en lo que pueda asistirte?

Recuerda que puedo:
- üìä Generar predicciones de demanda
- üîç Comparar modelos de ML
- üìà Analizar tendencias de ventas"""

        # Respuesta por defecto m√°s inteligente
        else:
            # Detectar si menciona productos o n√∫meros
            if any(word in user_input_lower for word in ['producto', 'item', 'art√≠culo']) or any(char.isdigit() for char in user_input):
                return """Parece que mencionas productos espec√≠ficos. 

Para ayudarte mejor, puedo:
- üìä **Predecir demanda** de un producto espec√≠fico
- üîç **Comparar modelos** para encontrar el m√°s preciso
- üìà **Analizar tendencias** de categor√≠as

¬øPodr√≠as especificar qu√© tipo de an√°lisis necesitas?

**Ejemplo:** "Predice la demanda del producto 1 para los pr√≥ximos 30 d√≠as" """

            else:
                return """No estoy seguro de c√≥mo ayudarte con esa consulta espec√≠fica, pero puedo asistirte con:

üéØ **An√°lisis de Demanda:**
- Predicciones para productos espec√≠ficos
- Comparaci√≥n de modelos de ML
- An√°lisis de tendencias y patrones

üí° **Prueba pregunt√°ndome:**
- "¬øQu√© modelo es m√°s preciso?"
- "Predice la demanda del producto X"
- "Analiza las tendencias de ventas"

¬øEn qu√© puedo ayudarte?"""

    def render_chat_input(self):
        """Renderizar input del chat"""
        try:
            # Input del usuario
            user_input = st.chat_input("Escribe tu consulta sobre predicci√≥n de demanda...")
            
            if user_input:
                # Agregar mensaje del usuario
                st.session_state.messages.append({
                    "role": "user",
                    "content": user_input,
                    "timestamp": datetime.now()
                })
                
                # Procesar mensaje y obtener respuesta
                response = self._process_user_message(user_input)
                
                # Agregar respuesta del asistente
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response,
                    "timestamp": datetime.now()
                })
                
                st.rerun()
                
        except Exception as e:
            st.error(f"Error en chat input: {str(e)}")
            logger.error(f"Error en render_chat_input(): {e}")

    def run(self):
        """Ejecutar la aplicaci√≥n principal"""
        try:
            self.render_sidebar()
            
            # T√≠tulo principal
            st.title("ü§ñ MicroAnalytics - Chat de Predicci√≥n")
            st.markdown("*An√°lisis inteligente de demanda para micronegocios*")
            
            # Mostrar historial de chat
            for message in st.session_state.messages:
                self._render_message(message)
            
            # Input del usuario
            self.render_chat_input()
            
        except Exception as e:
            st.error(f"Error en la aplicaci√≥n: {str(e)}")
            logger.error(f"Error en run(): {e}")


def main():
    """Funci√≥n principal"""
    try:
        # Configurar logging
        logging.basicConfig(level=logging.INFO)
        
        # Crear y ejecutar la aplicaci√≥n
        app = ChatbotFrontend()
        app.run()
        
    except Exception as e:
        st.error(f"Error cr√≠tico: {str(e)}")
        logger.error(f"Error cr√≠tico en main(): {e}")


if __name__ == "__main__":
    main()