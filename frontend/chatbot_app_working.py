"""
Frontend para Ollama - Interfaz de Chat para PredicciÃ³n de Demanda
================================================================

AplicaciÃ³n Streamlit que proporciona una interfaz de usuario moderna
para interactuar con el sistema de predicciÃ³n de demanda a travÃ©s de Ollama.
"""

import streamlit as st
import asyncio
import json
import requests
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import sys
import os

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Agregar el directorio padre al path para importar mÃ³dulos
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Importaciones bÃ¡sicas sin Ollama por ahora
try:
    from chatbot.communication_schema import (
        PredictionScope, ModelType, RequestType
    )
    OLLAMA_AVAILABLE = True
except ImportError as e:
    print(f"Ollama integration not available: {e}")
    OLLAMA_AVAILABLE = False
    # Definir enums bÃ¡sicos como fallback
    class PredictionScope:
        SINGLE_PRODUCT = "single_product"
        CATEGORY = "category" 
        BUSINESS = "business"
        MARKET = "market"
    
    class ModelType:
        AUTO_SELECT = "auto_select"
        LINEAR = "linear"
        POLYNOMIAL = "polynomial"


class ChatbotFrontend:
    """Clase principal para el frontend del chatbot"""
    
    def __init__(self):
        self.backend_url = "http://localhost:8000"  # URL del backend FastAPI
        self.session_id = self._get_session_id()
        self.ollama_client = None
        self.interpreter = None
        
        # Inicializar estado de la sesiÃ³n
        if 'messages' not in st.session_state:
            st.session_state.messages = []
        if 'prediction_history' not in st.session_state:
            st.session_state.prediction_history = []
        if 'current_context' not in st.session_state:
            st.session_state.current_context = {}
        if 'tool_results' not in st.session_state:
            st.session_state.tool_results = {
                'recent_predictions': [],
                'recent_comparisons': [],
                'recent_analysis': [],
                'last_action': None,
                'last_results': None
            }
    
    def _get_session_id(self) -> str:
        """Obtener o crear ID de sesiÃ³n"""
        if 'session_id' not in st.session_state:
            st.session_state.session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        return st.session_state.session_id
    
    async def _init_ollama_client(self):
        """Inicializar cliente de Ollama con detecciÃ³n automÃ¡tica de modelos"""
        if not OLLAMA_AVAILABLE:
            logger.warning("Ollama integration no disponible")
            return False
            
        if self.ollama_client is None:
            try:
                from chatbot.ollama_integration import OllamaClient, OllamaConfig
                
                # URL fija de Ollama
                ollama_url = "https://3200-34-168-28-225.ngrok-free.app"
                
                # Crear configuraciÃ³n temporal para detectar modelos
                temp_config = OllamaConfig(
                    base_url=ollama_url,
                    model_name="llama3.2",  # Modelo por defecto
                    timeout=30,
                    max_tokens=1000,
                    temperature=0.7
                )
                
                # Crear cliente temporal
                temp_client = OllamaClient(temp_config)
                
                # Detectar modelos disponibles
                available_models = await self._detect_available_models(temp_client, ollama_url)
                
                if not available_models:
                    logger.warning("No se pudieron detectar modelos en Ollama")
                    return False
                
                # Seleccionar el mejor modelo
                best_model = self._select_best_model(available_models)
                logger.info(f"Usando modelo: {best_model} de {available_models}")
                
                # Crear configuraciÃ³n final
                final_config = OllamaConfig(
                    base_url=ollama_url,
                    model_name=best_model,
                    timeout=120,
                    max_tokens=1000,
                    temperature=0.7
                )
                
                # Crear cliente final
                self.ollama_client = OllamaClient(final_config)
                
                # Verificar conexiÃ³n final
                if await self.ollama_client.check_connection():
                    logger.info(f"Ollama conectado exitosamente con modelo {best_model}")
                    st.session_state.ollama_connected = True
                    st.session_state.ollama_model_used = best_model
                    return True
                else:
                    logger.warning("No se pudo conectar con Ollama")
                    self.ollama_client = None
                    return False
                    
            except ImportError as e:
                logger.error(f"Error importando Ollama (posiblemente falta aiohttp): {e}")
                return False
            except Exception as e:
                logger.error(f"Error inicializando Ollama: {e}")
                self.ollama_client = None
                return False
        
        return True

    async def _detect_available_models(self, client, base_url):
        """Detectar modelos disponibles en Ollama"""
        try:
            # Intentar importar aiohttp
            try:
                import aiohttp
            except ImportError:
                logger.error("aiohttp no estÃ¡ disponible. InstÃ¡lalo con: pip install aiohttp")
                return []
            
            async with aiohttp.ClientSession() as session:
                headers = {
                    "ngrok-skip-browser-warning": "true",
                    "Content-Type": "application/json"
                }
                
                async with session.get(
                    f"{base_url}/api/tags",
                    timeout=aiohttp.ClientTimeout(total=10),
                    headers=headers
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        models = [model.get('name', '').split(':')[0] for model in data.get('models', [])]
                        models = [m for m in models if m]  # Filtrar nombres vacÃ­os
                        logger.info(f"Modelos detectados: {models}")
                        return models
                    else:
                        logger.warning(f"Error al obtener modelos: {response.status}")
                        return []
        except Exception as e:
            logger.error(f"Error detectando modelos: {e}")
            # Fallback con modelos comunes
            return ['llama3.2', 'llama3.1', 'llama3']

    def _select_best_model(self, available_models):
        """Seleccionar el mejor modelo basado en prioridad"""
        # Prioridad de modelos (del mejor al menos preferido)
        priority_models = [
            'llama3.2',
            'llama3.1', 
            'llama3',
            'llama2',
            'codellama',
            'mistral',
            'gemma',
            'phi',
            'qwen'
        ]
        
        # Buscar el primer modelo de la lista de prioridad que estÃ© disponible
        for preferred in priority_models:
            for available in available_models:
                if preferred.lower() in available.lower():
                    return available
        
        # Si no se encuentra ninguno preferido, usar el primero disponible
        if available_models:
            return available_models[0]
        
        # Fallback
        return "llama3.2"
    
    def render_sidebar(self):
        """Renderizar barra lateral con configuraciones"""
        st.sidebar.header("ğŸ¤– ConfiguraciÃ³n del Chat")
        
        with st.sidebar.container():
            st.markdown('<div class="sidebar-section">', unsafe_allow_html=True)
            st.subheader("ğŸ”— Estado de Ollama")
            
            # Estado de conexiÃ³n
            if st.session_state.get('ollama_connected', False):
                st.success(f"âœ… Conectado - Modelo: {st.session_state.get('ollama_model_used', 'N/A')}")
            else:
                st.warning("âš ï¸ No conectado")
            
            # URL de Ollama (fija)
            st.info("ğŸŒ URL: https://3200-34-168-28-225.ngrok-free.app")
            
            # BotÃ³n para reconectar
            if st.button("ğŸ”„ Reconectar Ollama"):
                self.ollama_client = None
                st.session_state.ollama_connected = False
                try:
                    connected = asyncio.run(self._init_ollama_client())
                    if connected:
                        st.success("âœ… Reconectado exitosamente")
                        st.rerun()
                    else:
                        st.error("âŒ Error al reconectar")
                except Exception as e:
                    st.error(f"âŒ Error: {e}")
            
            st.markdown('</div>', unsafe_allow_html=True)
            
            # ConfiguraciÃ³n de predicciones
            st.markdown('<div class="sidebar-section">', unsafe_allow_html=True)
            st.subheader("ğŸ“Š ConfiguraciÃ³n de Predicciones")
            
            # DÃ­as para predicciÃ³n
            prediction_days = st.slider(
                "DÃ­as a predecir",
                min_value=7,
                max_value=90,
                value=st.session_state.get('prediction_days', 30),
                step=7,
                help="NÃºmero de dÃ­as hacia el futuro para las predicciones"
            )
            st.session_state.prediction_days = prediction_days
            
            # Incluir intervalos de confianza
            include_confidence = st.checkbox(
                "Incluir intervalos de confianza",
                value=st.session_state.get('include_confidence', True),
                help="Mostrar rangos de confianza en las predicciones"
            )
            st.session_state.include_confidence = include_confidence
            
            # Usar cachÃ©
            use_cache = st.checkbox(
                "Usar cachÃ© de modelos",
                value=st.session_state.get('use_cache', True),
                help="Acelerar predicciones usando modelos en cachÃ©"
            )
            st.session_state.use_cache = use_cache
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # Historial de predicciones
        st.sidebar.header("ï¿½ Historial")
        
        with st.sidebar.container():
            st.markdown('<div class="sidebar-section">', unsafe_allow_html=True)
            
            # Alcance de predicciÃ³n
            scope_options = {
                "Producto Ãºnico": PredictionScope.SINGLE_PRODUCT,
                "CategorÃ­a": PredictionScope.CATEGORY,
                "Negocio": PredictionScope.BUSINESS,
                "Mercado": PredictionScope.MARKET
            }
            
            selected_scope = st.selectbox(
                "Alcance",
                options=list(scope_options.keys()),
                help="Alcance de la predicciÃ³n"
            )
            st.session_state.prediction_scope = scope_options[selected_scope]
            
            # DÃ­as de predicciÃ³n
            prediction_days = st.slider(
                "DÃ­as a predecir",
                min_value=1,
                max_value=365,
                value=30,
                help="NÃºmero de dÃ­as hacia el futuro"
            )
            st.session_state.prediction_days = prediction_days
            
            # Tipo de modelo
            model_options = {
                "Auto-selecciÃ³n": ModelType.AUTO_SELECT,
                "Lineal": ModelType.LINEAR,
                "Polinomial": ModelType.POLYNOMIAL
            }
            
            selected_model_type = st.selectbox(
                "Tipo de modelo",
                options=list(model_options.keys()),
                help="Modelo de ML a utilizar"
            )
            st.session_state.model_type = model_options[selected_model_type]
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # Historial de predicciones
        st.sidebar.header("ğŸ“ˆ Historial")
        
        if st.session_state.prediction_history:
            for i, pred in enumerate(st.session_state.prediction_history[-5:]):
                with st.sidebar.expander(f"PredicciÃ³n {i+1}"):
                    st.write(f"**Fecha:** {pred['timestamp']}")
                    st.write(f"**Confianza:** {pred['confidence']:.1%}")
                    st.write(f"**Modelo:** {pred['model']}")
        else:
            st.sidebar.info("No hay predicciones aÃºn")
        
        # Limpiar historial
        if st.sidebar.button("ğŸ—‘ï¸ Limpiar Chat"):
            st.session_state.messages = []
            st.session_state.prediction_history = []
            st.rerun()
    
    def render_chat_interface(self):
        """Renderizar interfaz principal de chat"""
        st.header("ğŸ¤– Asistente de PredicciÃ³n de Demanda")
        st.markdown("Pregunta sobre predicciones, anÃ¡lisis de tendencias y insights de tu negocio.")
        
        # Contenedor para mensajes
        chat_container = st.container()
        
        with chat_container:
            # Mostrar historial de mensajes
            for message in st.session_state.messages:
                self._render_message(message)
        
        # Input para nuevos mensajes
        user_input = st.chat_input("Escribe tu pregunta sobre predicciÃ³n de demanda...")
        
        if user_input:
            # Agregar mensaje del usuario
            st.session_state.messages.append({
                "role": "user",
                "content": user_input,
                "timestamp": datetime.now()
            })
            
            # Procesar mensaje
            with st.spinner("Analizando y generando respuesta..."):
                response = self._process_user_message(user_input)
            
            # Agregar respuesta del asistente
            st.session_state.messages.append({
                "role": "assistant",
                "content": response,
                "timestamp": datetime.now()
            })
            
            st.rerun()
    
    def _render_message(self, message: Dict[str, Any]):
        """Renderizar un mensaje individual"""
        timestamp = message['timestamp'].strftime("%H:%M")
        
        if message['role'] == 'user':
            st.markdown(f"""
            <div class="stChatMessage user-message">
                <strong>TÃº ({timestamp}):</strong> {message['content']}
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
            <div class="stChatMessage assistant-message">
                <strong>Asistente ({timestamp}):</strong> {message['content']}
            </div>
            """, unsafe_allow_html=True)
            
            # Si hay datos de predicciÃ³n, mostrar visualizaciÃ³n
            if 'prediction_data' in message:
                self._render_prediction_visualization(message['prediction_data'])
    
    def _render_prediction_visualization(self, prediction_data: Dict[str, Any]):
        """Renderizar visualizaciÃ³n de predicciÃ³n"""
        try:
            if not prediction_data or 'predicciones' not in prediction_data:
                return
            
            # Crear grÃ¡fico simple con Plotly
            import plotly.graph_objects as go
            
            predicciones = prediction_data.get('predicciones', [])
            if not predicciones:
                return
                
            dias = list(range(1, len(predicciones) + 1))
            
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=dias,
                y=predicciones,
                mode='lines+markers',
                name='PredicciÃ³n',
                line=dict(color='#1976d2', width=3),
                marker=dict(size=6)
            ))
            
            fig.update_layout(
                title=f"PredicciÃ³n para Producto {prediction_data.get('producto_id', 'N/A')}",
                xaxis_title="DÃ­as",
                yaxis_title="Demanda",
                height=400,
                template="plotly_white"
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
        except Exception as e:
            logger.warning(f"Error renderizando visualizaciÃ³n: {e}")

    def _process_user_message(self, user_input: str) -> str:
        """Procesar mensaje del usuario y generar respuesta"""
        
        # Intentar inicializar Ollama si no estÃ¡ disponible
        if not self.ollama_client and OLLAMA_AVAILABLE:
            try:
                # Usar asyncio.run para la inicializaciÃ³n
                asyncio.run(self._init_ollama_client())
            except Exception as e:
                logger.warning(f"No se pudo inicializar Ollama: {e}")
        
        # PRIMERO: Detectar si es una pregunta de seguimiento
        if self._detect_follow_up_question(user_input):
            logger.info(f"Pregunta de seguimiento detectada: {user_input}")
            return self._get_contextual_response(user_input)
        
        # SEGUNDO: Detectar intenciÃ³n del usuario para nuevas consultas
        intent = self._detect_intent(user_input)
        
        logger.info(f"Intent detectado: {intent} para input: {user_input}")
        
        # Manejar segÃºn la intenciÃ³n
        if intent == 'prediction':
            return self._handle_prediction_request(user_input)
        elif intent == 'comparison':
            return self._handle_model_comparison(user_input)
        elif intent == 'analysis':
            return self._handle_analysis_request(user_input)
        else:  # general
            return self._handle_general_chat(user_input)
    
    def _detect_intent(self, user_input: str) -> str:
        """Detectar la intenciÃ³n del usuario de manera mÃ¡s inteligente"""
        user_input_lower = user_input.lower()
        
        # Palabras clave mÃ¡s especÃ­ficas para cada intenciÃ³n
        prediction_keywords = [
            'predecir', 'predicciÃ³n', 'pronÃ³stico', 'demanda', 'ventas futuras', 'futuro',
            'prÃ³ximos', 'dÃ­as', 'semanas', 'meses', 'cuÃ¡nto', 'cuÃ¡ntos',
            'producto', 'cuÃ¡l serÃ¡', 'cuanto voy a vender', 'proyecciÃ³n',
            'estimaciÃ³n', 'forecast', 'prever', 'proyectar'
        ]
        
        # Palabras especÃ­ficas para comparaciÃ³n (mÃ¡s restrictivas)
        comparison_keywords = [
            'comparar modelos', 'mejor modelo', 'quÃ© modelo', 'cuÃ¡l modelo',
            'modelo mÃ¡s preciso', 'modelo mÃ¡s exacto', 'accuracy', 'precisiÃ³n del modelo',
            'performance modelo', 'rendimiento modelo', 'evaluar modelos',
            'algoritmo mejor', 'ml accuracy'
        ]
        
        # Palabras para anÃ¡lisis de tendencias
        analysis_keywords = [
            'analizar tendencia', 'tendencia de ventas', 'patrÃ³n', 'insight',
            'estudiar', 'examinar tendencia', 'revisar tendencia', 'investigar patrÃ³n',
            'reportar tendencia', 'reporte', 'anÃ¡lisis histÃ³rico', 'comportamiento',
            'categorÃ­a', 'anÃ¡lisis de categorÃ­a'
        ]
        
        # ConversaciÃ³n general
        general_keywords = [
            'hola', 'buenos', 'buenas', 'hey', 'hi', 'hello', 'quÃ© haces',
            'que haces', 'gracias', 'thanks', 'ayuda', 'help', 'quÃ© puedes',
            'capacidades', 'como funciona', 'cÃ³mo funciona'
        ]
        
        # DetecciÃ³n especÃ­fica por frases exactas (mayor prioridad)
        if any(phrase in user_input_lower for phrase in ['comparar modelos', 'quÃ© modelo', 'cuÃ¡l modelo', 'mejor modelo']):
            return 'comparison'
        
        if any(phrase in user_input_lower for phrase in ['analizar tendencia', 'tendencia de', 'anÃ¡lisis de']):
            return 'analysis'
        
        # Buscar con nÃºmeros de producto (indica predicciÃ³n)
        import re
        if re.search(r'producto\s+\d+', user_input_lower) or re.search(r'\d+\s+dÃ­as?', user_input_lower):
            return 'prediction'
        
        if re.search(r'demanda.*producto', user_input_lower) or re.search(r'cuÃ¡l serÃ¡.*demanda', user_input_lower):
            return 'prediction'
        
        # Contar coincidencias por categorÃ­a
        prediction_count = sum(1 for keyword in prediction_keywords if keyword in user_input_lower)
        comparison_count = sum(1 for keyword in comparison_keywords if keyword in user_input_lower)
        analysis_count = sum(1 for keyword in analysis_keywords if keyword in user_input_lower)
        general_count = sum(1 for keyword in general_keywords if keyword in user_input_lower)
        
        # Decidir basado en la mayor cantidad de coincidencias
        counts = {
            'prediction': prediction_count,
            'comparison': comparison_count,
            'analysis': analysis_count,
            'general': general_count
        }
        
        max_count = max(counts.values())
        if max_count == 0:
            return 'general'  # Si no hay coincidencias claras, asumir conversaciÃ³n general
        
        # Retornar la intenciÃ³n con mÃ¡s coincidencias
        for intent, count in counts.items():
            if count == max_count:
                return intent
        
        return 'general'
    
    def _save_tool_result(self, tool_type: str, result_data: Dict[str, Any], user_input: str):
        """Guardar resultados de herramientas para contexto futuro"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        if tool_type == 'prediction':
            prediction_summary = {
                'timestamp': timestamp,
                'user_query': user_input,
                'producto_id': result_data.get('producto_id', 'N/A'),
                'prediccion_promedio': result_data.get('predicciones', [0])[-1] if result_data.get('predicciones') else 0,
                'modelo_usado': result_data.get('mejor_modelo', 'N/A'),
                'confianza': result_data.get('confianza', 0),
                'tendencia': result_data.get('trend_type', 'N/A'),
                'raw_data': result_data
            }
            st.session_state.tool_results['recent_predictions'].append(prediction_summary)
            # Mantener solo las Ãºltimas 3
            if len(st.session_state.tool_results['recent_predictions']) > 3:
                st.session_state.tool_results['recent_predictions'].pop(0)
        
        elif tool_type == 'comparison':
            comparison_summary = {
                'timestamp': timestamp,
                'user_query': user_input,
                'mejor_modelo': result_data.get('best_model', 'Random Forest'),
                'r2_scores': result_data.get('r2_scores', {}),
                'conclusion': result_data.get('conclusion', 'ComparaciÃ³n realizada'),
                'raw_data': result_data
            }
            st.session_state.tool_results['recent_comparisons'].append(comparison_summary)
            # Mantener solo las Ãºltimas 2
            if len(st.session_state.tool_results['recent_comparisons']) > 2:
                st.session_state.tool_results['recent_comparisons'].pop(0)
        
        elif tool_type == 'analysis':
            analysis_summary = {
                'timestamp': timestamp,
                'user_query': user_input,
                'tipo_analisis': result_data.get('analysis_type', 'general'),
                'categoria': result_data.get('category', 'N/A'),
                'insights': result_data.get('insights', []),
                'raw_data': result_data
            }
            st.session_state.tool_results['recent_analysis'].append(analysis_summary)
            # Mantener solo las Ãºltimas 2
            if len(st.session_state.tool_results['recent_analysis']) > 2:
                st.session_state.tool_results['recent_analysis'].pop(0)
        
        # Actualizar Ãºltimo resultado
        st.session_state.tool_results['last_action'] = tool_type
        st.session_state.tool_results['last_results'] = result_data

    def _detect_follow_up_question(self, user_input: str) -> bool:
        """Detectar si es una pregunta de seguimiento sobre resultados anteriores"""
        follow_up_patterns = [
            'quÃ© puedes decir al respecto',
            'quÃ© opinas sobre esto',
            'quÃ© significa esto',
            'explÃ­came esto',
            'analiza esto',
            'interpreta esto',
            'quÃ© conclusiones',
            'quÃ© recomendaciones',
            'basÃ¡ndote en esto',
            'sobre estos resultados',
            'sobre esta informaciÃ³n',
            'que me dices de',
            'cÃ³mo interpretas',
            'quÃ© piensas'
        ]
        
        user_lower = user_input.lower()
        return any(pattern in user_lower for pattern in follow_up_patterns)

    def _get_contextual_response(self, user_input: str) -> str:
        """Generar respuesta contextual basada en resultados anteriores"""
        last_action = st.session_state.tool_results.get('last_action')
        last_results = st.session_state.tool_results.get('last_results')
        
        if not last_action or not last_results:
            return self._get_intelligent_fallback(user_input)
        
        user_lower = user_input.lower()
        
        if last_action == 'comparison':
            return self._analyze_model_comparison_context(user_input, last_results)
        elif last_action == 'prediction':
            return self._analyze_prediction_context(user_input, last_results)
        elif last_action == 'analysis':
            return self._analyze_analysis_context(user_input, last_results)
        
        return self._get_intelligent_fallback(user_input)

    def _analyze_model_comparison_context(self, user_input: str, comparison_data: Dict) -> str:
        """Analizar y explicar resultados de comparaciÃ³n de modelos"""
        try:
            # Obtener el Ãºltimo resultado de comparaciÃ³n
            recent_comp = st.session_state.tool_results['recent_comparisons'][-1] if st.session_state.tool_results['recent_comparisons'] else None
            
            if not recent_comp:
                return "No encontrÃ© resultados de comparaciÃ³n recientes para analizar."
            
            response = f"""
            <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #7b1fa2; color: #000000;">
            
            ## ğŸ¤” AnÃ¡lisis de la ComparaciÃ³n de Modelos
            
            **BasÃ¡ndome en tu pregunta:** "{user_input}"
            
            ### ğŸ¯ InterpretaciÃ³n de Resultados
            
            <div style="background-color: #e8f5e8; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #4caf50; color: #1b5e20; font-weight: 600;">
            ğŸ† **Modelo Ganador:** {recent_comp['mejor_modelo']}
            </div>
            
            ### ğŸ“Š Â¿QuÃ© significan estos nÃºmeros?
            
            **RÂ² Score (Coeficiente de DeterminaciÃ³n):**
            - ğŸ“ˆ Mide quÃ© tan bien el modelo explica la variabilidad de tus datos
            - ğŸ¯ Rango: 0.0 (terrible) a 1.0 (perfecto)
            - âœ… **Mayor RÂ² = Mejor predicciÃ³n**
            
            **MSE (Error CuadrÃ¡tico Medio):**
            - ğŸ“‰ Promedio de errores al cuadrado
            - ğŸ¯ **Menor MSE = Mejor precisiÃ³n**
            
            **MAE (Error Absoluto Medio):**
            - ğŸ“Š Error promedio sin elevar al cuadrado
            - ğŸ¯ **Menor MAE = Predicciones mÃ¡s cercanas**
            
            ### ğŸ’¡ Recomendaciones PrÃ¡cticas
            
            <div style="background-color: #f3e5f5; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #9c27b0; color: #4a148c; font-weight: 600;">
            ğŸš€ **Para tu negocio:** El {recent_comp['mejor_modelo']} te darÃ¡ las predicciones mÃ¡s confiables
            </div>
            
            **PrÃ³ximos pasos sugeridos:**
            1. ğŸ“Š Usa el {recent_comp['mejor_modelo']} para predicciones futuras
            2. ğŸ”„ Re-evalÃºa mensualmente con nuevos datos
            3. ğŸ“ˆ Monitora la precisiÃ³n en la prÃ¡ctica
            
            ### ğŸ¤– Â¿Quieres que realice algÃºn anÃ¡lisis especÃ­fico?
            
            Puedo ayudarte con:
            - ğŸ“Š PredicciÃ³n de demanda usando el mejor modelo
            - ğŸ“ˆ AnÃ¡lisis de tendencias especÃ­ficas
            - ğŸ¯ Recomendaciones personalizadas para tu inventario
            
            </div>
            """
            
            return response
            
        except Exception as e:
            return f"Error analizando la comparaciÃ³n: {str(e)}"

    def _analyze_prediction_context(self, user_input: str, prediction_data: Dict) -> str:
        """Analizar y explicar resultados de predicciÃ³n"""
        try:
            recent_pred = st.session_state.tool_results['recent_predictions'][-1] if st.session_state.tool_results['recent_predictions'] else None
            
            if not recent_pred:
                return "No encontrÃ© predicciones recientes para analizar."
            
            response = f"""
            <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #1976d2; color: #000000;">
            
            ## ğŸ“Š AnÃ¡lisis de tu PredicciÃ³n de Demanda
            
            **Respondiendo a:** "{user_input}"
            
            ### ğŸ¯ Resumen de Resultados
            
            <div style="background-color: #e3f2fd; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #1976d2; color: #0d47a1; font-weight: 600;">
            ğŸ“¦ **Producto {recent_pred['producto_id']}:** {recent_pred['prediccion_promedio']:.1f} unidades promedio
            </div>
            
            ### ğŸ” InterpretaciÃ³n Detallada
            
            **Confianza del Modelo:**
            - ğŸ¯ **{recent_pred['confianza']:.1%}** de confianza en la predicciÃ³n
            - ğŸ¤– Modelo usado: **{recent_pred['modelo_usado']}**
            - ğŸ“ˆ Tendencia detectada: **{recent_pred['tendencia']}**
            
            ### ğŸ’¡ Â¿QuÃ© significa para tu negocio?
            
            **PlanificaciÃ³n de Inventario:**
            """
            
            # Agregar recomendaciones basadas en la predicciÃ³n
            prediccion = recent_pred['prediccion_promedio']
            if prediccion > 100:
                response += """
            <div style="background-color: #fff3e0; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #ff9800; color: #e65100; font-weight: 600;">
            ğŸ”¥ **Alta demanda proyectada** - Considera aumentar tu inventario
            </div>
            """
            elif prediccion > 50:
                response += """
            <div style="background-color: #e8f5e8; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #4caf50; color: #1b5e20; font-weight: 600;">
            âœ… **Demanda moderada** - MantÃ©n niveles de stock normales
            </div>
            """
            else:
                response += """
            <div style="background-color: #fce4ec; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #e91e63; color: #880e4f; font-weight: 600;">
            ğŸ“‰ **Demanda baja** - EvalÃºa promociones o reducir inventario
            </div>
            """
            
            response += f"""
            ### ğŸ“‹ Acciones Recomendadas
            
            1. ğŸ“Š **Stock Ã³ptimo:** {prediccion * 1.2:.0f} unidades (20% buffer)
            2. ğŸ”„ **Punto de reorden:** {prediccion * 0.3:.0f} unidades
            3. ğŸ“… **PrÃ³xima revisiÃ³n:** En 1 semana
            
            ### ğŸ¤– Â¿Te ayudo con algo mÃ¡s?
            
            Puedo ayudarte a:
            - ğŸ” Comparar con otros productos
            - ğŸ“ˆ Analizar tendencias histÃ³ricas
            - ğŸ’° Calcular rentabilidad proyectada
            
            </div>
            """
            
            return response
            
        except Exception as e:
            return f"Error analizando la predicciÃ³n: {str(e)}"

    def _analyze_analysis_context(self, user_input: str, analysis_data: Dict) -> str:
        """Analizar y explicar resultados de anÃ¡lisis general"""
        try:
            recent_analysis = st.session_state.tool_results['recent_analysis'][-1] if st.session_state.tool_results['recent_analysis'] else None
            
            if not recent_analysis:
                return "No encontrÃ© anÃ¡lisis recientes para interpretar."
            
            response = f"""
            <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #ff9800; color: #000000;">
            
            ## ğŸ“ˆ InterpretaciÃ³n del AnÃ¡lisis
            
            **Tu consulta:** "{user_input}"
            
            ### ğŸ¯ Resumen del AnÃ¡lisis Realizado
            
            <div style="background-color: #fff3e0; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #ff9800; color: #e65100; font-weight: 600;">
            ğŸ“Š **Tipo:** {recent_analysis['tipo_analisis']} | **CategorÃ­a:** {recent_analysis['categoria']}
            </div>
            
            ### ğŸ’¡ Insights Clave
            """
            
            for insight in recent_analysis.get('insights', [])[:3]:  # Mostrar mÃ¡ximo 3
                response += f"""
            - ğŸ¯ {insight}
            """
            
            response += """
            
            ### ğŸš€ PrÃ³ximos Pasos Sugeridos
            
            1. ğŸ“Š Monitorear tendencias identificadas
            2. ğŸ”„ Ajustar estrategias segÃºn insights
            3. ğŸ“ˆ Revisar resultados en 2 semanas
            
            ### ğŸ¤– Â¿Necesitas mÃ¡s detalles?
            
            Puedo profundizar en:
            - ğŸ“Š AnÃ¡lisis especÃ­ficos por producto
            - ğŸ” Comparaciones detalladas
            - ğŸ’¡ Recomendaciones personalizadas
            
            </div>
            """
            
            return response
            
        except Exception as e:
            return f"Error analizando el anÃ¡lisis: {str(e)}"

    def _handle_prediction_request(self, user_input: str) -> str:
        """Manejar solicitud de predicciÃ³n"""
        try:
            # Extraer parÃ¡metros de la sesiÃ³n
            product_id = self._extract_product_id(user_input)
            
            # Crear peticiÃ³n (formato compatible con backend)
            request_data = {
                "producto_id": product_id,
                "dias_adelante": st.session_state.get('prediction_days', 30),
                "include_confidence": True,
                "use_cache": True
            }
            
            # Intentar llamar al backend
            try:
                response = requests.post(
                    f"{self.backend_url}/api/predict/demanda",
                    json=request_data,
                    timeout=5  # Timeout corto para demo
                )
                
                if response.status_code == 200:
                    prediction_data = response.json()
                    
                    # Guardar en historial
                    st.session_state.prediction_history.append({
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
                        "confidence": prediction_data.get('confianza', 0),
                        "model": prediction_data.get('mejor_modelo', 'unknown'),
                        "data": prediction_data
                    })
                    
                    # Guardar resultado para contexto futuro
                    self._save_tool_result('prediction', prediction_data, user_input)
                    
                    # Generar respuesta interpretativa
                    return self._generate_prediction_interpretation(prediction_data, user_input)
                else:
                    # Backend responde pero hay error, usar datos simulados
                    return self._generate_demo_prediction(product_id, user_input)
                    
            except (requests.RequestException, requests.Timeout):
                # Backend no disponible, usar datos simulados
                return self._generate_demo_prediction(product_id, user_input)
                
        except Exception as e:
            return f"Error procesando predicciÃ³n: {str(e)}"
    
    def _generate_demo_prediction(self, product_id: int, user_input: str) -> str:
        """Generar predicciÃ³n de demostraciÃ³n cuando el backend no estÃ¡ disponible"""
        import numpy as np
        import hashlib
        
        # Usar hash del input para generar datos Ãºnicos pero consistentes
        seed_value = int(hashlib.md5(f"{product_id}_{user_input}".encode()).hexdigest()[:8], 16) % 10000
        np.random.seed(seed_value)
        
        days = st.session_state.get('prediction_days', 30)
        
        # Crear predicciÃ³n simulada mÃ¡s variada
        base_demand = 30 + (product_id % 20) * 5 + np.random.randint(-10, 20)
        
        # Diferentes tipos de tendencia basados en el producto
        trend_types = ['creciente', 'decreciente', 'estacional', 'estable']
        trend_type = trend_types[product_id % 4]
        
        predicted_values = []
        for i in range(days):
            if trend_type == 'creciente':
                trend_value = base_demand + (i * 0.5) + np.random.normal(0, 2)
            elif trend_type == 'decreciente':
                trend_value = base_demand - (i * 0.3) + np.random.normal(0, 2)
            elif trend_type == 'estacional':
                # PatrÃ³n semanal
                seasonal = 15 * np.sin(2 * np.pi * i / 7) + 5 * np.cos(2 * np.pi * i / 14)
                trend_value = base_demand + seasonal + np.random.normal(0, 3)
            else:  # estable
                trend_value = base_demand + np.random.normal(0, 4)
            
            value = max(1, int(trend_value))
            predicted_values.append(value)
        
        # Calcular mÃ©tricas mÃ¡s realistas
        avg_demand = np.mean(predicted_values)
        trend = trend_type
        
        # Confidence basada en tipo de tendencia
        confidence_base = {
            'estable': 0.85,
            'creciente': 0.78,
            'decreciente': 0.72,
            'estacional': 0.68
        }
        
        confidence = confidence_base[trend_type] + np.random.uniform(-0.08, 0.08)
        confidence = max(0.6, min(0.95, confidence))
        
        # Seleccionar modelo basado en tendencia
        model_selection = {
            'estable': 'linear',
            'creciente': 'linear', 
            'decreciente': 'polynomial',
            'estacional': 'random_forest'
        }
        
        selected_model = model_selection[trend_type]
        
        prediction_data = {
            'predicciones': predicted_values,
            'confianza': confidence,
            'mejor_modelo': selected_model,
            'dias_adelante': days,
            'producto_id': product_id,
            'trend_type': trend_type
        }
        
        # Guardar resultados en el contexto de herramientas
        self._save_tool_result('prediction', prediction_data, user_input)
        
        # Guardar en historial
        st.session_state.prediction_history.append({
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
            "confidence": confidence,
            "model": selected_model,
            "trend": trend_type,
            "data": prediction_data
        })
        
        # Agregar nota de demostraciÃ³n
        demo_note = "ğŸ“ **Modo Demo**: Datos simulados (backend en desarrollo)"
        interpretation = self._generate_prediction_interpretation(prediction_data, user_input)
        
        return f"{demo_note}\n\n{interpretation}"

    def _handle_model_comparison(self, user_input: str) -> str:
        """Manejar comparaciÃ³n de modelos con fallback inteligente"""
        try:
            # Intentar obtener comparaciÃ³n del backend
            comparison_id = f"comp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            response = requests.get(
                f"{self.backend_url}/api/predict/models/comparison/{comparison_id}",
                timeout=5  # Timeout corto
            )
            
            if response.status_code == 200:
                comparison_data = response.json()
                
                # Guardar resultado para contexto futuro
                self._save_tool_result('comparison', comparison_data, user_input)
                
                return self._generate_comparison_interpretation(comparison_data, user_input)
            else:
                # Si falla el backend, generar comparaciÃ³n simulada
                return self._generate_demo_comparison(user_input)
                
        except Exception as e:
            logger.warning(f"Error en comparaciÃ³n del backend: {e}")
            return self._generate_demo_comparison(user_input)

    def _generate_demo_comparison(self, user_input: str) -> str:
        """Generar comparaciÃ³n de modelos simulada"""
        import numpy as np
        
        # Datos simulados de comparaciÃ³n
        models_comparison = {
            'linear': {
                'mse': np.random.uniform(0.15, 0.25),
                'r2': np.random.uniform(0.75, 0.85),
                'mae': np.random.uniform(0.12, 0.18),
                'speed': 'Muy rÃ¡pido',
                'complexity': 'Baja'
            },
            'polynomial': {
                'mse': np.random.uniform(0.10, 0.20),
                'r2': np.random.uniform(0.80, 0.90),
                'mae': np.random.uniform(0.08, 0.15),
                'speed': 'Moderado',
                'complexity': 'Media'
            },
            'random_forest': {
                'mse': np.random.uniform(0.08, 0.15),
                'r2': np.random.uniform(0.85, 0.95),
                'mae': np.random.uniform(0.06, 0.12),
                'speed': 'Lento',
                'complexity': 'Alta'
            }
        }
        
        # Encontrar el mejor modelo por RÂ²
        best_model = max(models_comparison.keys(), key=lambda k: models_comparison[k]['r2'])
        
        # Guardar resultados en el contexto
        comparison_result = {
            'best_model': best_model.replace('_', ' ').title(),
            'r2_scores': {k: v['r2'] for k, v in models_comparison.items()},
            'models_data': models_comparison,
            'conclusion': f"{best_model.replace('_', ' ').title()} es el mÃ¡s preciso"
        }
        self._save_tool_result('comparison', comparison_result, user_input)
        
        interpretation = f"""
        <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #e0e0e0; color: #000000;">
        
        ## ğŸ” ComparaciÃ³n de Modelos de ML
        
        **BasÃ¡ndome en tu consulta:** "{user_input}"
        
        ### ğŸ† Modelo Recomendado: **{best_model.replace('_', ' ').title()}**
        
        ### ğŸ“Š Resultados Detallados
        
        """
        
        for model, metrics in models_comparison.items():
            model_name = model.replace('_', ' ').title()
            r2_color = "#1b5e20" if metrics['r2'] > 0.85 else "#f57c00" if metrics['r2'] > 0.75 else "#d32f2f"
            
            interpretation += f"""
        **ğŸ¤– {model_name}:**
        - **RÂ² Score**: <span style="color: {r2_color}; font-weight: bold; background-color: #f5f5f5; padding: 2px 6px; border-radius: 4px;">{metrics['r2']:.3f}</span>
        - **MSE**: <span style="color: #000000; font-weight: bold;">{metrics['mse']:.3f}</span>
        - **MAE**: <span style="color: #000000; font-weight: bold;">{metrics['mae']:.3f}</span>
        - **Velocidad**: <span style="color: #4a148c; font-weight: bold;">{metrics['speed']}</span>
        - **Complejidad**: <span style="color: #4a148c; font-weight: bold;">{metrics['complexity']}</span>
        
        """
        
        interpretation += f"""
        ### ğŸ’¡ Recomendaciones
        
        <div style="background-color: #e8f5e8; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #4caf50; color: #1b5e20; font-weight: 600;">
        ğŸ¯ **Mejor opciÃ³n:** {best_model.replace('_', ' ').title()} con RÂ² de {models_comparison[best_model]['r2']:.3f}
        </div>
        
        <div style="background-color: #f3e5f5; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #9c27b0; color: #4a148c; font-weight: 600;">
        ğŸ“ˆ **Criterio de selecciÃ³n:** Mayor RÂ² indica mejor capacidad predictiva
        </div>
        
        ### ğŸ“‹ GuÃ­a de SelecciÃ³n:
        - **Linear**: Ideal para tendencias simples y predicciones rÃ¡pidas
        - **Polynomial**: Mejor para patrones mÃ¡s complejos
        - **Random Forest**: MÃ¡xima precisiÃ³n para datos complejos
        
        </div>
        """
        
        return interpretation
    
    def _handle_analysis_request(self, user_input: str) -> str:
        """Manejar solicitud de anÃ¡lisis con capacidades mejoradas"""
        
        # Extraer informaciÃ³n de la consulta
        category = self._extract_category_from_input(user_input)
        
        if category:
            return self._generate_category_analysis(category, user_input)
        else:
            return self._generate_general_analysis(user_input)

    def _extract_category_from_input(self, user_input: str) -> str:
        """Extraer categorÃ­a del input del usuario"""
        user_input_lower = user_input.lower()
        
        categories = {
            'electrÃ³nicos': ['electronico', 'electronica', 'electronic', 'tecnologia'],
            'ropa': ['ropa', 'vestimenta', 'clothing', 'textil'],
            'alimentaciÃ³n': ['alimento', 'comida', 'food', 'bebida'],
            'hogar': ['hogar', 'casa', 'home', 'domestico'],
            'deportes': ['deporte', 'sport', 'fitness', 'ejercicio'],
            'salud': ['salud', 'health', 'medicina', 'farmacia'],
            'belleza': ['belleza', 'beauty', 'cosmetico', 'maquillaje']
        }
        
        for category, keywords in categories.items():
            if any(keyword in user_input_lower for keyword in keywords):
                return category
        
        return None

    def _generate_category_analysis(self, category: str, user_input: str) -> str:
        """Generar anÃ¡lisis de tendencias para una categorÃ­a especÃ­fica"""
        import numpy as np
        
        # Generar datos simulados para la categorÃ­a
        np.random.seed(hash(category) % 1000)
        
        # Simular tendencias de los Ãºltimos 6 meses
        months = ['Enero', 'Febrero', 'Marzo', 'Abril', 'Mayo', 'Junio']
        base_sales = np.random.randint(1000, 5000)
        
        # Generar tendencia (creciente, decreciente o estable)
        trend_type = np.random.choice(['creciente', 'decreciente', 'estable'])
        
        sales_data = []
        for i in range(6):
            if trend_type == 'creciente':
                variation = 1 + (i * 0.1) + np.random.uniform(-0.05, 0.05)
            elif trend_type == 'decreciente':
                variation = 1 - (i * 0.08) + np.random.uniform(-0.05, 0.05)
            else:  # estable
                variation = 1 + np.random.uniform(-0.1, 0.1)
            
            sales_data.append(int(base_sales * variation))
        
        # Calcular estadÃ­sticas
        avg_sales = np.mean(sales_data)
        growth_rate = ((sales_data[-1] - sales_data[0]) / sales_data[0]) * 100
        
        trend_color = "#1b5e20" if growth_rate > 5 else "#d32f2f" if growth_rate < -5 else "#f57c00"
        trend_bg = "#e8f5e8" if growth_rate > 5 else "#ffebee" if growth_rate < -5 else "#fff3e0"
        
        interpretation = f"""
        <div style="background-color: #ffffff; padding: 20px; border-radius: 12px; border: 2px solid #e0e0e0; color: #000000;">
        
        ## ğŸ“ˆ AnÃ¡lisis de Tendencias: {category.title()}
        
        **Consulta analizada:** "{user_input}"
        
        ### ğŸ“Š Resumen Ejecutivo
        
        <div style="background-color: {trend_bg}; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid {trend_color.replace('#', '').replace('1b5e20', '#4caf50').replace('d32f2f', '#f44336').replace('f57c00', '#ff9800')};">
        <strong style="color: {trend_color};">Tendencia General: {trend_type.title()}</strong><br>
        <strong style="color: #000000;">Crecimiento: {growth_rate:+.1f}%</strong><br>
        <strong style="color: #000000;">Ventas Promedio: {avg_sales:,.0f} unidades/mes</strong>
        </div>
        
        ### ğŸ“… Datos HistÃ³ricos (Ãšltimos 6 Meses)
        
        """
        
        for i, (month, sales) in enumerate(zip(months, sales_data)):
            change = ""
            if i > 0:
                change_pct = ((sales - sales_data[i-1]) / sales_data[i-1]) * 100
                change_color = "#1b5e20" if change_pct > 0 else "#d32f2f"
                change = f" <span style='color: {change_color}; font-weight: bold;'>({change_pct:+.1f}%)</span>"
            
            interpretation += f"- **{month}**: <span style='color: #000000; font-weight: bold;'>{sales:,} unidades</span>{change}\n"
        
        interpretation += f"""
        
        ### ğŸ” Insights Clave
        
        """
        
        if trend_type == 'creciente':
            interpretation += f"""
        - ğŸ“ˆ **Crecimiento sostenido**: La categorÃ­a {category} muestra una tendencia positiva
        - ğŸ¯ **Oportunidad**: Considera aumentar inventario gradualmente
        - ğŸ’¡ **Estrategia**: Aprovecha el momentum con campaÃ±as de marketing
        """
        elif trend_type == 'decreciente':
            interpretation += f"""
        - ğŸ“‰ **Declive observado**: La categorÃ­a {category} estÃ¡ perdiendo tracciÃ³n
        - âš ï¸ **Alerta**: Revisa estrategias de precio y promociÃ³n
        - ğŸ”„ **AcciÃ³n**: Considera diversificar o renovar productos
        """
        else:
            interpretation += f"""
        - ğŸ“Š **Estabilidad**: La categorÃ­a {category} mantiene ventas consistentes
        - ğŸ¯ **Oportunidad**: Mercado maduro ideal para optimizaciÃ³n
        - ğŸ’¡ **Estrategia**: EnfÃ³cate en eficiencia y mÃ¡rgenes
        """
        
        interpretation += f"""
        
        ### ğŸ’¡ Recomendaciones EspecÃ­ficas
        
        <div style="background-color: #f3e5f5; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #9c27b0; color: #4a148c; font-weight: 600;">
        ğŸ¯ **PrÃ³ximos pasos:** Usa predicciones especÃ­ficas por producto para planificar inventario
        </div>
        
        <div style="background-color: #e3f2fd; padding: 15px; border-radius: 8px; margin: 15px 0; border: 2px solid #2196f3; color: #0d47a1; font-weight: 600;">
        ğŸ“Š **Tip:** Pregunta "Â¿CuÃ¡l serÃ¡ la demanda del producto X en los prÃ³ximos 30 dÃ­as?" para anÃ¡lisis especÃ­fico
        </div>
        
        </div>
        """
        
        return interpretation

    def _handle_general_chat(self, user_input: str) -> str:
        """Manejar chat general con Ollama"""
        try:
            # Si hay cliente Ollama disponible, usar IA para responder
            if self.ollama_client:
                # Crear contexto sobre las herramientas disponibles
                system_context = """
                Eres un asistente especializado en predicciÃ³n de demanda y anÃ¡lisis de micronegocios.
                
                HERRAMIENTAS DISPONIBLES:
                1. PredicciÃ³n de demanda: Puedo predecir la demanda futura de productos especÃ­ficos
                2. ComparaciÃ³n de modelos: Puedo comparar diferentes modelos de ML para encontrar el mÃ¡s preciso
                3. AnÃ¡lisis de tendencias: Puedo analizar patrones en los datos histÃ³ricos
                4. Reportes de inventario: Puedo generar reportes sobre estado del inventario
                
                IMPORTANTE: 
                - Si el usuario pregunta sobre predicciones, demanda, ventas futuras o productos especÃ­ficos, sugiere usar las herramientas de predicciÃ³n
                - Si pregunta sobre modelos o precisiÃ³n, sugiere comparar modelos
                - Para conversaciÃ³n general, responde de manera amigable y Ãºtil
                - Siempre ofrece ayuda especÃ­fica relacionada con el negocio
                
                Responde de manera conversacional y Ãºtil. Si detectas que necesitan usar alguna herramienta especÃ­fica, guÃ­alos hacia esa funcionalidad.
                """
                
                # Usar asyncio para la llamada a Ollama
                try:
                    response = asyncio.run(self._get_ollama_response(user_input, system_context))
                    return response
                except Exception as e:
                    logger.warning(f"Error con Ollama: {e}")
                    return self._get_intelligent_fallback(user_input)
            else:
                return self._get_intelligent_fallback(user_input)
                
        except Exception as e:
            return f"Disculpa, hubo un error procesando tu mensaje. Â¿PodrÃ­as reformular tu pregunta?"

    async def _get_ollama_response(self, user_input: str, system_context: str) -> str:
        """Obtener respuesta de Ollama de manera asÃ­ncrona"""
        try:
            if not self.ollama_client:
                return self._get_intelligent_fallback(user_input)
            
            # Preparar el contexto de conversaciÃ³n
            conversation_context = self._build_conversation_context()
            
            # Crear el prompt completo
            full_prompt = f"""
            {system_context}
            
            CONTEXTO DE CONVERSACIÃ“N:
            {conversation_context}
            
            USUARIO: {user_input}
            
            ASISTENTE: """
            
            # Llamar a Ollama con los parÃ¡metros correctos
            response = await self.ollama_client.generate_response(
                prompt=full_prompt,
                session_id=self.session_id,
                include_ml_context=True,
                stream=False
            )
            
            return response.strip()
            
        except Exception as e:
            logger.error(f"Error en respuesta de Ollama: {e}")
            return self._get_intelligent_fallback(user_input)

    def _build_conversation_context(self) -> str:
        """Construir contexto de conversaciÃ³n enriquecido para Mistral"""
        if not st.session_state.messages:
            return "Esta es una nueva conversaciÃ³n."
        
        # Tomar los Ãºltimos 5 mensajes para contexto mÃ¡s rico
        recent_messages = st.session_state.messages[-5:] if len(st.session_state.messages) > 5 else st.session_state.messages
        
        context_parts = []
        
        # Construir contexto basado en mensajes
        for msg in recent_messages:
            role = "Usuario" if msg['role'] == 'user' else "Asistente"
            content = msg['content']
            
            # Limitar longitud del contenido para contexto
            content_summary = content[:200] + "..." if len(content) > 200 else content
            context_parts.append(f"{role}: {content_summary}")
        
        context = "\n".join(context_parts)
        
        # Agregar informaciÃ³n sobre resultados recientes de herramientas
        tool_results = st.session_state.tool_results
        
        if (tool_results['recent_predictions'] or 
            tool_results['recent_comparisons'] or 
            tool_results['recent_analysis']):
            
            context += "\n\n=== RESULTADOS RECIENTES DE HERRAMIENTAS ==="
            
            # Predicciones recientes
            if tool_results['recent_predictions']:
                context += "\n\nPREDICCIONES REALIZADAS:"
                for pred in tool_results['recent_predictions'][-2:]:  # Ãšltimas 2
                    context += f"\n- Producto {pred['producto_id']}: {pred['prediccion_promedio']:.1f} unidades, modelo {pred['modelo_usado']}, confianza {pred['confianza']:.1%}, tendencia {pred['tendencia']}"
            
            # Comparaciones recientes
            if tool_results['recent_comparisons']:
                context += "\n\nCOMPARACIONES DE MODELOS:"
                for comp in tool_results['recent_comparisons'][-1:]:  # Ãšltima comparaciÃ³n
                    context += f"\n- Modelo ganador: {comp['mejor_modelo']}"
                    context += f"\n- ConclusiÃ³n: {comp['conclusion']}"
            
            # AnÃ¡lisis recientes
            if tool_results['recent_analysis']:
                context += "\n\nANÃLISIS REALIZADOS:"
                for analysis in tool_results['recent_analysis'][-1:]:
                    context += f"\n- Tipo: {analysis['tipo_analisis']} para {analysis['categoria']}"
            
            # InformaciÃ³n sobre la Ãºltima acciÃ³n
            if tool_results['last_action']:
                context += f"\n\nÃšLTIMA ACCIÃ“N: {tool_results['last_action']}"
        
        return context

    def _get_intelligent_fallback(self, user_input: str) -> str:
        """Respuesta inteligente de fallback basada en anÃ¡lisis del input"""
        user_input_lower = user_input.lower()
        
        # Saludos y conversaciÃ³n general
        if any(word in user_input_lower for word in ['hola', 'buenos', 'buenas', 'hey', 'hi']):
            return """Â¡Hola! ğŸ‘‹ Soy tu asistente de anÃ¡lisis de demanda para micronegocios. 

Puedo ayudarte con:
- ğŸ“Š **Predicciones de demanda** para productos especÃ­ficos
- ğŸ” **ComparaciÃ³n de modelos** para encontrar el mÃ¡s preciso
- ğŸ“ˆ **AnÃ¡lisis de tendencias** en tus datos de ventas
- ğŸ“‹ **Reportes de inventario** y recomendaciones

Â¿En quÃ© puedo ayudarte hoy?"""

        # Preguntas sobre capacidades
        elif any(word in user_input_lower for word in ['quÃ© puedes', 'que haces', 'ayuda', 'help', 'capacidades']):
            return """ğŸ¤– **Mis Capacidades:**

**ğŸ“Š PredicciÃ³n de Demanda:**
- Predigo ventas futuras de productos especÃ­ficos
- Uso mÃºltiples modelos de ML para mayor precisiÃ³n
- Proporciono intervalos de confianza

**ğŸ” AnÃ¡lisis Avanzado:**
- Comparo diferentes modelos para encontrar el mÃ¡s preciso
- Identifico tendencias y patrones estacionales
- Genero insights accionables para tu negocio

ğŸ’¡ **Ejemplos de lo que puedes preguntarme:**
- "Â¿CuÃ¡l serÃ¡ la demanda del producto 1 en los prÃ³ximos 30 dÃ­as?"
- "Â¿QuÃ© modelo es mÃ¡s preciso para mis productos?"
- "Analiza las tendencias de ventas del Ãºltimo mes"

Â¿QuÃ© te gustarÃ­a explorar?"""

        # Agradecimientos
        elif any(word in user_input_lower for word in ['gracias', 'thanks', 'thank you']):
            return """Â¡De nada! ğŸ˜Š 

Estoy aquÃ­ para ayudarte con el anÃ¡lisis de tu negocio. Â¿Hay algo mÃ¡s en lo que pueda asistirte?

Recuerda que puedo:
- ğŸ“Š Generar predicciones de demanda
- ğŸ” Comparar modelos de ML
- ğŸ“ˆ Analizar tendencias de ventas"""

        # Respuesta por defecto mÃ¡s inteligente
        else:
            # Detectar si menciona productos o nÃºmeros
            if any(word in user_input_lower for word in ['producto', 'item', 'artÃ­culo']) or any(char.isdigit() for char in user_input):
                return """Parece que mencionas productos especÃ­ficos. 

Para ayudarte mejor, puedo:
- ğŸ“Š **Predecir demanda** de un producto especÃ­fico
- ğŸ” **Comparar modelos** para encontrar el mÃ¡s preciso
- ğŸ“ˆ **Analizar tendencias** de categorÃ­as

Â¿PodrÃ­as especificar quÃ© tipo de anÃ¡lisis necesitas?

**Ejemplo:** "Predice la demanda del producto 1 para los prÃ³ximos 30 dÃ­as" """

            else:
                return """No estoy seguro de cÃ³mo ayudarte con esa consulta especÃ­fica, pero puedo asistirte con:

ğŸ¯ **AnÃ¡lisis de Demanda:**
- Predicciones para productos especÃ­ficos
- ComparaciÃ³n de modelos de ML
- AnÃ¡lisis de tendencias y patrones

ğŸ’¡ **Prueba preguntÃ¡ndome:**
- "Â¿QuÃ© modelo es mÃ¡s preciso?"
- "Predice la demanda del producto X"
- "Analiza las tendencias de ventas"

Â¿En quÃ© puedo ayudarte?"""

    def render_chat_input(self):
        """Renderizar input del chat"""
        try:
            # Input del usuario
            user_input = st.chat_input("Escribe tu consulta sobre predicciÃ³n de demanda...")
            
            if user_input:
                # Agregar mensaje del usuario
                st.session_state.messages.append({
                    "role": "user",
                    "content": user_input,
                    "timestamp": datetime.now()
                })
                
                # Procesar mensaje y obtener respuesta
                response = self._process_user_message(user_input)
                
                # Agregar respuesta del asistente
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response,
                    "timestamp": datetime.now()
                })
                
                st.rerun()
                
        except Exception as e:
            st.error(f"Error en chat input: {str(e)}")
            logger.error(f"Error en render_chat_input(): {e}")

    def run(self):
        """Ejecutar la aplicaciÃ³n principal"""
        # CSS optimizado para mÃ¡xima legibilidad y contraste
        st.markdown("""
        <style>
        /* Reset y base */
        .main {
            padding-top: 1rem;
        }

        /* Estilos base para mensajes del chat con contraste mÃ¡ximo */
        .stChatMessage {
            border-radius: 12px;
            padding: 20px;
            margin: 15px 0;
            font-size: 16px;
            line-height: 1.7;
            border: 2px solid transparent;
        }

        .user-message {
            background-color: #ffffff !important;
            border: 3px solid #1976d2 !important;
            color: #0d47a1 !important;
            font-weight: 600;
            box-shadow: 0 3px 10px rgba(25, 118, 210, 0.2);
        }

        .user-message strong {
            color: #0d47a1 !important;
            font-weight: 700;
        }

        .assistant-message {
            background-color: #ffffff !important;
            border: 3px solid #7b1fa2 !important;
            color: #212121 !important;
            box-shadow: 0 4px 15px rgba(123, 31, 162, 0.15);
        }

        /* Forzar color negro en todos los elementos del asistente */
        .assistant-message,
        .assistant-message *,
        .assistant-message p,
        .assistant-message div,
        .assistant-message span,
        .assistant-message li,
        .assistant-message td,
        .assistant-message th {
            color: #000000 !important;
            font-weight: 500 !important;
        }

        /* TÃ­tulos con contraste extremo */
        .assistant-message h1,
        .assistant-message h2,
        .assistant-message h3,
        .assistant-message h4,
        .assistant-message h5,
        .assistant-message h6 {
            color: #000000 !important;
            font-weight: 800 !important;
            margin: 20px 0 15px 0 !important;
            text-shadow: none !important;
            background-color: #e8eaf6 !important;
            padding: 8px 12px !important;
            border-radius: 6px !important;
            border-left: 4px solid #3f51b5 !important;
        }

        /* Texto enfatizado negro sÃ³lido */
        .assistant-message strong,
        .assistant-message b {
            color: #000000 !important;
            font-weight: 800 !important;
            background-color: #fff3e0 !important;
            padding: 2px 4px !important;
            border-radius: 3px !important;
        }

        /* Enlaces completamente visibles */
        .assistant-message a {
            color: #000000 !important;
            text-decoration: underline !important;
            font-weight: 700 !important;
            background-color: #e3f2fd !important;
            padding: 2px 4px !important;
            border-radius: 3px !important;
        }

        /* Indicadores de confianza con contraste extremo */
        .confidence-high { 
            color: #000000 !important; 
            font-weight: 900 !important;
            background-color: #c8e6c9 !important;
            padding: 6px 12px !important;
            border-radius: 6px !important;
            border: 3px solid #2e7d32 !important;
            text-shadow: none !important;
        }

        .confidence-medium { 
            color: #000000 !important; 
            font-weight: 900 !important;
            background-color: #ffe0b2 !important;
            padding: 6px 12px !important;
            border-radius: 6px !important;
            border: 3px solid #f57c00 !important;
            text-shadow: none !important;
        }

        .confidence-low { 
            color: #000000 !important; 
            font-weight: 900 !important;
            background-color: #ffcdd2 !important;
            padding: 6px 12px !important;
            border-radius: 6px !important;
            border: 3px solid #d32f2f !important;
            text-shadow: none !important;
        }

        /* Listas completamente negras */
        .assistant-message ul,
        .assistant-message ol {
            color: #000000 !important;
            margin: 15px 0 !important;
        }

        .assistant-message ul li,
        .assistant-message ol li {
            color: #000000 !important;
            font-weight: 600 !important;
            margin: 8px 0 !important;
            padding-left: 10px !important;
        }

        .assistant-message ul li::marker,
        .assistant-message ol li::marker {
            color: #000000 !important;
        }

        /* CÃ³digo completamente visible */
        .assistant-message code,
        .assistant-message pre {
            background-color: #f5f5f5 !important;
            color: #000000 !important;
            border: 2px solid #666666 !important;
            border-radius: 4px !important;
            padding: 8px !important;
            font-weight: 600 !important;
        }

        /* Forzar estilos en todos los elementos de Streamlit */
        div[data-testid="stMarkdown"],
        div[data-testid="stMarkdown"] *,
        .stMarkdown,
        .stMarkdown *,
        .st-emotion-cache-acwcvw,
        .st-emotion-cache-acwcvw *,
        .st-emotion-cache-1sdpuyj,
        .st-emotion-cache-1sdpuyj * {
            color: #000000 !important;
            font-weight: 500 !important;
        }

        /* Selectores especÃ­ficos para elementos problemÃ¡ticos */
        .assistant-message div[data-testid="stMarkdown"] p,
        .assistant-message div[data-testid="stMarkdown"] div,
        .assistant-message div[data-testid="stMarkdown"] span,
        .assistant-message div[data-testid="stMarkdown"] li,
        .assistant-message .stMarkdown p,
        .assistant-message .stMarkdown div,
        .assistant-message .stMarkdown span,
        .assistant-message .stMarkdown li {
            color: #000000 !important;
            font-weight: 500 !important;
        }

        /* Sidebar mejorado */
        .sidebar-section {
            margin-bottom: 30px;
            padding: 20px;
            background-color: #ffffff;
            border-radius: 10px;
            border: 2px solid #e0e0e0;
            color: #000000 !important;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .sidebar-section * {
            color: #000000 !important;
        }

        /* Forzar todo a negro como Ãºltimo recurso */
        .assistant-message [class*="st-emotion"],
        .assistant-message [class*="e1rzn78k"],
        .assistant-message [class*="erovr38"] {
            color: #000000 !important;
        }

        /* Asegurar que elementos especÃ­ficos sean negros */
        div.stChatMessage.assistant-message * {
            color: #000000 !important;
        }

        /* Override para cualquier clase que Streamlit pueda agregar */
        .assistant-message [class] {
            color: #000000 !important;
        }

        /* Modo oscuro deshabilitado para mÃ¡ximo contraste */
        @media (prefers-color-scheme: dark) {
            .assistant-message,
            .assistant-message * {
                background-color: #ffffff !important;
                color: #000000 !important;
            }
        }
        </style>
        """, unsafe_allow_html=True)

        try:
            self.render_sidebar()
            
            # TÃ­tulo principal
            st.title("ğŸ¤– MicroAnalytics - Chat de PredicciÃ³n")
            st.markdown("*AnÃ¡lisis inteligente de demanda para micronegocios*")
            
            # Mostrar historial de chat
            for message in st.session_state.messages:
                self._render_message(message)
            
            # Input del usuario
            self.render_chat_input()
            
        except Exception as e:
            st.error(f"Error en la aplicaciÃ³n: {str(e)}")
            logger.error(f"Error en run(): {e}")


def main():
    """FunciÃ³n principal"""
    try:
        # Configurar logging
        logging.basicConfig(level=logging.INFO)
        
        # Crear y ejecutar la aplicaciÃ³n
        app = ChatbotFrontend()
        app.run()
        
    except Exception as e:
        st.error(f"Error crÃ­tico: {str(e)}")
        logger.error(f"Error crÃ­tico en main(): {e}")


if __name__ == "__main__":
    main()